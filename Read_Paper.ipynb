{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd8875a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\William\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "警告呼叫API失敗\n",
      "警告呼叫API失敗\n",
      "警告呼叫API失敗\n",
      "警告呼叫API失敗\n",
      "警告呼叫API失敗\n",
      "警告呼叫API失敗\n",
      "警告呼叫API失敗\n",
      "警告呼叫API失敗\n",
      "警告呼叫API失敗\n",
      "警告呼叫API失敗\n",
      "警告呼叫API失敗\n",
      "警告呼叫API失敗\n",
      "警告呼叫API失敗\n",
      "警告呼叫API失敗\n",
      "警告呼叫API失敗\n",
      "警告呼叫API失敗\n",
      "警告呼叫API失敗\n",
      "警告呼叫API失敗\n",
      "警告呼叫API失敗\n",
      "警告呼叫API失敗\n",
      "警告呼叫API失敗\n",
      "警告呼叫API失敗\n",
      "警告呼叫API失敗\n",
      "警告呼叫API失敗\n",
      "警告呼叫API失敗\n",
      "警告呼叫API失敗\n",
      "警告呼叫API失敗\n",
      "警告呼叫API失敗\n",
      "警告呼叫API失敗\n",
      "警告呼叫API失敗\n",
      "警告呼叫API失敗\n",
      "警告呼叫API失敗\n",
      "警告呼叫API失敗\n",
      "警告呼叫API失敗\n",
      "警告呼叫API失敗\n",
      "警告呼叫API失敗\n",
      "警告呼叫API失敗\n",
      "警告呼叫API失敗\n",
      "警告呼叫API失敗\n",
      "警告呼叫API失敗\n",
      "警告呼叫API失敗\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'translation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 48\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     46\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m警告呼叫API失敗\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 48\u001b[0m \u001b[43mtranslation\u001b[49m\u001b[38;5;241m.\u001b[39mto_csv(pdf_name\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtranslation\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8-sig\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     50\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# 計算程式運行時間\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'translation' is not defined"
     ]
    }
   ],
   "source": [
    "from pypdf import PdfReader\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "import openai\n",
    "import pandas as pd\n",
    "import time\n",
    "start_time = time.time()\n",
    "openai.api_key = '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'\n",
    "nltk.download('punkt')\n",
    "\n",
    "pdf_name = \"Order-Free RNN with Visual Attention for Multi-Label Classification.pdf\" #@param {type:\"string\"}\n",
    "reader = PdfReader(pdf_name)\n",
    "number_of_pages = len(reader.pages)\n",
    "\n",
    "chunks = []\n",
    "\n",
    "for i in range(number_of_pages):\n",
    "    page = reader.pages[i]\n",
    "    text = page.extract_text()\n",
    "    sentences = sent_tokenize(text)\n",
    "    input_sentences = ''\n",
    "  \n",
    "    for sentence in sentences:\n",
    "        input_sentences += sentence\n",
    "        if len(input_sentences) > 1000:\n",
    "            chunks.append(input_sentences)\n",
    "            input_sentences = ''\n",
    "    chunks.append(input_sentences)\n",
    "    \n",
    "\n",
    "        \n",
    "for i in range(len(chunks)):\n",
    "    try:\n",
    "        completion = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"請你成為文章翻譯的小幫手，請協助翻譯以下技術文件，以繁體中文輸出\"},\n",
    "                    {\"role\": \"user\", \"content\": chunks[i]},\n",
    "                  ])\n",
    "        print('='*20)                                           \n",
    "        print('原文:\\n', chunks[i])\n",
    "        print('='*20)  \n",
    "        print('翻譯結果:\\n',completion.choices[0].message.content)\n",
    "        print('='*20)\n",
    "    except:\n",
    "        print(\"警告呼叫API失敗\")\n",
    "        \n",
    "translation.to_csv(pdf_name+\"translation\"+\".csv\", encoding='utf-8-sig')\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "# 計算程式運行時間\n",
    "duration = end_time - start_time\n",
    "print(f\"程式運行時間：{duration:.2f}秒\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "933f9ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\William\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "原文:\n",
      " IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS 1\n",
      "Unsupervised Maritime Vessel Re-Identification\n",
      "With Multi-Level Contrastive Learning\n",
      "Qian Zhang\n",
      " , Mingxin Zhang, Graduate Student Member, IEEE , Jinghe Liu, Xuanyu He\n",
      " ,\n",
      "Ran Song\n",
      " ,Member, IEEE , and Wei Zhang\n",
      " ,Member, IEEE\n",
      "Abstract — Re-identification (re-ID) of maritime vessels plays\n",
      "an important role in marine surveillance, but remains highly\n",
      "unexplored due to the lack of large-scale annotated datasets.In vessel re-ID, contrastive methods are supposed to learn\n",
      "discriminative representation from unlabeled vessel images in\n",
      "an unsupervised manner.However, directly introducing classical\n",
      "instance-level contrastive methods to maritime vessel re-ID suf-\n",
      "fers from the difficulty of finding vessel images with the same\n",
      "pseudo label as positive images, which potentially leads to inef-\n",
      "ficient training and unsatisfactory performance.This paper pro-\n",
      "poses a simple but effective method to solve such a hard positive\n",
      "problem.Our method takes all images in an intra-batch cluster\n",
      "as positives and excludes them from the set of negative samples\n",
      "when computing instance-level contrastive loss.\n",
      "====================\n",
      "翻譯結果:\n",
      " 智能交通系統IEEE交易期刊1 研究論文\n",
      "多級對比學習的無監督海上船隻再識別 \n",
      "作者: 張謙，張明欣，劉景和，何選玉，宋然，張威\n",
      "\n",
      "摘要 - 海上船隻的再識別在海上監察中扮演著重要角色，但由於缺乏大規模的標註數據集，此領域一直未被深入探索。在船隻再識別中，對比方法可以在無監督的情況下從未標註的船隻圖像中學習區分性表示。然而，直接引入經典實例級對比方法進行海上船隻再識別往往存在正樣本尋找困難的問題，這可能導致訓練低效和性能不理想。本文提出一種簡單而有效的方法來解決這個困難的正樣本問題。我們的方法將一個批次內的所有圖像作為正樣本，在計算實例級對比損失時將其從負樣本集合中排除。\n",
      "====================\n",
      "====================\n",
      "原文:\n",
      " Based on this\n",
      "strategy, we construct a multi-level contrastive learning (MCL)\n",
      "framework for vessel re-ID trained with the specifically designed\n",
      "intra-batch cluster-level contrastive loss along with the instance-\n",
      "level one.Experiments on a newly proposed dataset consisting of\n",
      "1,248 vessel identities show that MCL achieves the state-of-the-\n",
      "art performance compared with other unsupervised methods.Index Terms— Vessel re-identification, contrastive learning,\n",
      "unsupervised learning.I. I NTRODUCTION\n",
      "SIMILAR to\n",
      "person or vehicle re-identification (re-ID),\n",
      "maritime vessel re-ID aims to distinguish the vessels\n",
      "that share the same identity with the given query vessel\n",
      "among a number of vessels.It plays an important role in\n",
      "offshore surveillance and reconnaissance.For example, the\n",
      "re-ID of a vessel across different cameras and locations helps\n",
      "a lot to analyse its moving trajectory and speed.However,\n",
      "unlike persons and vehicles that can be easily captured by\n",
      "street CCTV cameras, it is expensive to collect and annotate\n",
      "vessel images, which leads to the lack of publicly available\n",
      "large-scale dataset for maritime vessel re-ID.\n",
      "====================\n",
      "翻譯結果:\n",
      " 基於此策略，我們構建了一個多層對比學習（MCL）框架，用於訓練具有特定設計的批內集群級對比損失和實例級損失的船舶重新識別。對包含1,248個船舶身份的新提出的數據集進行的實驗表明，相比其他無監督方法，MCL實現了最先進的性能。關鍵詞：船舶重新識別，對比學習，無監督學習。I.引言與人或車輛重新識別（重新識別）相似，海上船舶重新識別旨在區分與給定查詢船舶具有相同身份的船舶中的船舶。它在海上監視和偵察中發揮重要作用。例如，跨不同攝像頭和位置進行船舶重新識別可以很好地分析其移動軌跡和速度。然而，與可輕易通過街道閉路電視攝像機捕獲的人和車不同，收集和標註船舶圖像是昂貴的，這導致缺乏大型公開數據集用於海上船舶重新識別。\n",
      "====================\n",
      "====================\n",
      "原文:\n",
      " Moreover, most\n",
      "of the existing methods [1], [2], [3], [4], [5] handle the vessel\n",
      "re-ID in a supervised manner that has a strong reliance on\n",
      "Manuscript received 6 May 2022; revised 15 August 2022 and 14 December\n",
      "2022; accepted 23 January 2023.This work was supported in part by the\n",
      "National Natural Science Foundation of China under Grant 61991411 and\n",
      "Grant U1913204, in part by the National Key Research and Development\n",
      "Plan of China under Grant 2021ZD0112002, in part by the Natural Science\n",
      "Foundation of Shandong Province for Distinguished Young Scholars under\n",
      "Grant ZR2020JQ29, and in part by the Project for Self-Developed Innovation\n",
      "Team of Jinan City under Grant 2021GXRC038.The Associate Editor for\n",
      "this article was H. Gao.(Corresponding author: Wei Zhang.)The authors are with the School of Control Science and Engineering,\n",
      "Shandong University, Jinan 250061, China (e-mail: davidzhang@sdu.edu.cn).Digital Object Identifier 10.1109/TITS.2023.3243591\n",
      "Fig.1.Comparison between vessel images and person/vehicle images.\n",
      "====================\n",
      "翻譯結果:\n",
      " 此外，現有的大多數方法[1]，[2]，[3]，[4]，[5]在處理船舶重新識別方面都是以監督方式進行的，這強烈依賴於人員和設備的標註。本文於2022年5月6日收到稿件，並於2022年8月15日和2022年12月14日進行了修訂，於2023年1月23日接受。此工作部分由中國國家自然科學基金會贊助，項目編號為61991411和U1913204，部分由中國國家重點研發計劃資助，項目編號為2021ZD0112002，部分由山東省自然科學基金會傑出青年學者資助，項目編號為ZR2020JQ29，還有一部分由濟南市自主創新團隊項目資助，項目編號為2021GXRC038。本文的副編輯為H. Gao。(通訊作者：張偉)作者們隸屬於山東大學控制科學與工程學院，郵箱為davidzhang@sdu.edu.cn。數字對象標識符號為10.1109/TITS.2023.3243591。圖1顯示了船舶圖像與人員/車輛圖像之間的比較。\n",
      "====================\n",
      "====================\n",
      "原文:\n",
      " From\n",
      "top to bottom, we show sample images in Market1501 [24], VeRi776 [25] and\n",
      "the proposed VesselReID, where the image frames in the same colour denote\n",
      "that the corresponding images share the same person/vehicle/vessel identity.labeled data.As a result, compared to the re-ID of persons [6],\n",
      "[7], [8], [9], [10], [11] and vehicles [12], [13], [14], maritime\n",
      "vessel re-ID is significantly unexplored.Recently, unsupervised learning methods have shown\n",
      "promising results in person and/or vehicle re-ID.Such methods\n",
      "typically generate pseudo labels by various mechanisms to\n",
      "discover the relationship between instances for identity-level\n",
      "representation learning [15], [16], [17], [18], [19], [20], [21],\n",
      "[22], [23].For example, instead of using the popular clus-\n",
      "tering strategy, MMCL [16] assigned multi-labels for each\n",
      "image, converting person re-ID to a multi-label classification\n",
      "task.However, MMCL started with index-labels at the initial\n",
      "epochs, where images with the same identity are regarded as\n",
      "instances of separate classes and spread apart during the train-\n",
      "ing.\n",
      "====================\n",
      "翻譯結果:\n",
      " 我們展示了Market1501 [24]、VeRi776 [25]和提出的VesselReID中的樣本圖像，從上到下依次排列。在相同顏色的圖像幀中，表示對應的圖像具有相同的人/車輛/船只身份標籤信息。相對於人[6]、[7]、[8]、[9]、[10]、[11]和車輛[12]、[13]、[14]的重新識別，海上船只重新識別顯著未被開發。最近，非監督式學習方法在人和/或車輛的重新識別方面表現出有希望的結果。這些方法通常通過各種機制生成偽標籤，以發現實例之間的關係，進行身份級別的表示學習[15]，[16]，[17]，[18]，[19]，[20]，[21]，[22]，[23]。例如，MMCL [16]採用了多標籤分配的方法，將每個圖像分配為多個標籤，將人重新識別轉換為多標籤分類任務。然而，MMCL在最初的幾個時期使用了索引標籤，其中具有相同身份的圖像被視為不同類的實例並在訓練期間分散開來。\n",
      "====================\n",
      "====================\n",
      "原文:\n",
      " Zheng et al.[21] proposed a viewpoint-aware progressive\n",
      "clustering (V APC) method to handle viewpoint variation for\n",
      "vehicle re-ID, where the re-ID model was also trained with\n",
      "index-labels for several epochs before the V APC was con-\n",
      "ducted.It can be concluded that generating pseudo-labels for\n",
      "unlabeled images is a prerequisite for unsupervised re-ID.Analogously, unsupervised vessel re-ID can also be\n",
      "achieved with high-quality pseudo-labels.However, there\n",
      "exists two distinctive features of maritime vessels, as shown in\n",
      "Fig.1, which make it more challenging to train vessel re-ID\n",
      "models in an unsupervised manner.First, different vessels\n",
      "belonging to the same super-category, such as passenger\n",
      "ship, present with similar appearances.Second, images of the\n",
      "same vessel captured from various viewpoints usually have\n",
      "large variations in topological structure and visual appearance.1558-0016 © 2023 IEEE.Personal use is permitted, but republication/redistribution requires IEEE permission.See https://www.ieee.org/publications/rights/index.html for more information.This article has been accepted for inclusion in a future issue of this journal.\n",
      "====================\n",
      "翻譯結果:\n",
      " Zheng等人[21]提出了一種視角感知漸進式聚類(V APC)方法來處理交通工具重新識別的視角變化，其中重新識別模型在進行V APC之前也使用索引標籤進行了幾個時期的訓練。可以總結出，為無標籤圖像生成偽標籤是無監督重新識別的先決條件。類比地，高質量的偽標籤也可以實現無監督的船舶重新識別。然而，如圖1所示，海上船舶具有兩個獨特的特徵，這使得以無監督的方式訓練船舶重新識別模型更具挑戰性。首先，屬於同一超類別（如客輪）的不同船舶呈現出相似的外觀。其次，從各種不同角度拍攝的同一艘船的圖像通常具有較大的拓撲結構和視覺外觀變化。1558-0016© 2023 IEEE。個人使用是允許的，但轉載/重新分發需要IEEE的許可。有關更多信息，請參見https://www.ieee.org/publications/rights/index.html。本文已被接受並包含在未來期刊的一個問題中。\n",
      "====================\n",
      "====================\n",
      "原文:\n",
      " Content is final as presented, with the exception of pagination.Authorized licensed use limited to: National Chung Hsing Univ.. Downloaded on March 27,2023 at 07:07:45 UTC from IEEE Xplore.Restrictions apply.\n",
      "====================\n",
      "翻譯結果:\n",
      " 內容即為最終版，除了分頁之外。授權許可的使用僅限於：國立中興大學。已於2023年3月27日07:07:45 UTC從IEEE Xplore下載。限制條件適用。\n",
      "====================\n",
      "====================\n",
      "原文:\n",
      " 2 IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS\n",
      "As such, it is challenging to generate accurate pseudo-labels\n",
      "for unlabeled vessel images by clustering.The inaccurate\n",
      "pseudo-labels should have made it hard to regard images\n",
      "with the same vessel identity as positives, known as the hard\n",
      "positive problem (HPP) in this paper.Even worse, although\n",
      "pseudo labels make it possible to learn feature representations\n",
      "with contrastive losses, it remains hard the discovery of true\n",
      "positive images with the same pseudo label as positives since\n",
      "conventional instance-level contrastive losses regard images\n",
      "with same pseudo label as separate classes.These two factors\n",
      "together make unsupervised vessel re-ID more sensitive to the\n",
      "HPP than other re-ID tasks.In fact, the HPP will lead to an uniform feature distribution\n",
      "[26] in the representation space and the identity-level repre-\n",
      "sentation learning will degenerate to an instance-level one.It is noteworthy that MMCL [16] used a specifically designed\n",
      "augmentation scheme to expand the real data distribution\n",
      "and SpCL [17] employed the PK sampler to avoid pushing\n",
      "a sample image away from other uncertain images in the\n",
      "representation space.\n",
      "====================\n",
      "翻譯結果:\n",
      " 因此，使用聚類生成未標記船只圖像的準確假標籤是具有挑戰性的。這些不準確的假標籤可能會使將具有相同船只身份的圖像視為正樣本變得困難，這在本文中稱為“困難的正樣本問題”（HPP）。更糟糕的是，即使假標籤使得使用對比損失學習特徵表示成為可能，發現具有相同假標籤的真正正樣本圖像仍然是困難的，因為傳統的實例級對比損失將帶有相同假標籤的圖像視為分開的類別。這兩個因素共同使得無監督船只再識別比其他再識別任務更加敏感於HPP。實際上，HPP會導致特徵空間中的統一特徵分佈[26]，並且身份級表示學習會退化為實例級表示學習。值得注意的是，MMCL [16] 使用了一個特別設計的增強方案來擴展真實數據分布，而SpCL [17] 則採用了PK採樣器，以避免將樣本圖像從表示空間中推開其他不確定圖像。\n",
      "====================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "原文:\n",
      " Both tactics effectively prevent from\n",
      "generating a uniform feature distribution at the early stage of\n",
      "contrastive learning and thus mitigate the HPP.However, both\n",
      "MMCL [16] and SpCL [17] fail in maritime vessel re-ID as\n",
      "it is more sensitive to the HPP than person and vehicle re-ID.To address the HPP, this paper proposes a multi-level\n",
      "contrastive learning (MCL) framework which conducts both\n",
      "instance-level and cluster-level discrimination for vessel re-ID.Note that ICE [19] incorporated both cluster-level and\n",
      "instance-level pseudo labels into contrastive learning where\n",
      "all person images with the same pseudo labels were partially\n",
      "represented by cluster centroids.As such, the positive images\n",
      "would not be pushed away from each other in the representa-\n",
      "tion space during the training.Different from the hard instance\n",
      "contrast that only considers the hardest positive pairs of\n",
      "person images in ICE, the proposed MCL employs all positive\n",
      "pairs in a mini-batch for instance-level contrastive learning to\n",
      "obtain more detailed representation of vessel images.\n",
      "====================\n",
      "翻譯結果:\n",
      " 這兩種策略有效地防止在對比學習的早期階段生成均勻的特徵分佈，因此減輕了熱點問題。然而，MMCL [16] 和 SpCL [17] 在海上船舶重新識別方面失敗了，因為它對熱點問題比人和車輛重新識別更敏感。為了解決熱點問題，本文提出了一個多級對比學習 (MCL) 框架，該框架為船舶重新識別進行了實例級和聚類級別的區別。需要注意的是，ICE [19] 將聚類級別和實例級別的擬合標籤納入了對比學習中，在其中具有相同擬合標籤的所有人物圖像均由聚類中心部分表示。因此，在訓練期間，正面影像不會在表示空間中互相推開。與 ICE 中僅考慮最難的人像正面配對的困難實例對比不同，本文提出的 MCL 採用一個小批次中的所有正面配對進行實例級對比學習，以獲得更詳細的船舶圖像表示。\n",
      "====================\n",
      "====================\n",
      "原文:\n",
      " Also,\n",
      "we design an intra-batch cluster-level discrimination module\n",
      "to avoid using the memory bank that restricts the application\n",
      "of ICE on large-scale datasets.In addition, considering the\n",
      "lack of a publicly available large-scale dataset for vessel re-ID,\n",
      "we create the VesselReID dataset to experimentally evaluate\n",
      "the vessel re-ID methods.The main contributions of this work are summarised as\n",
      "follows:\n",
      "1) We propose to take samples in the same intra-batch clus-\n",
      "ter as positives and remove them from negative samples when\n",
      "computing the contrastive loss.This strategy addresses the\n",
      "hard positive problem that exists in instance-level contrastive\n",
      "learning for vessel re-ID.2) We propose an intra-batch cluster-level contrastive loss\n",
      "that can be computed with no need of a memory bank, which\n",
      "can also mitigate the side effects of the hard positive problem\n",
      "to some extent.Based on it, we manage to establish the\n",
      "unsupervised MCL framework for vessel re-ID.3) We build VesselReID, a large-scale dataset expected to\n",
      "serve as a standard dataset for vessel re-ID.\n",
      "====================\n",
      "翻譯結果:\n",
      " 此外，\n",
      "我們設計了一個批內群體層級區分模塊，以避免使用限制在大規模數據集上應用ICE的內存銀行。此外，考慮到缺乏一個公開可用的大規模船舶再識別數據集，我們創建了VesselReID數據集來實驗評估船舶再識別方法。本研究的主要貢獻如下：\n",
      "1）我們建議將同一批內群體中的樣本作為正樣本進行取樣，並在計算對比損失時從負樣本中刪除它們。這種策略解決了船舶再識別實例級對比學習中存在的困難正樣本問題。 \n",
      "2）我們提出了一種批內群體層級的對比損失，可以在不需要內存銀行的情況下計算，也可以在一定程度上緩解困難正樣本問題的副作用。基於它，我們成功建立了船舶再識別的無監督MCL框架。\n",
      "3）我們建立了VesselReID，一個大規模數據集，預期成為船舶再識別的標準數據集。\n",
      "====================\n",
      "====================\n",
      "原文:\n",
      " And extensive\n",
      "experiments conducted on VesselReID demonstrate that theproposed MCL outperforms the state-of-the-art methods for\n",
      "unsupervised vessel re-ID.II.R ELATED WORK\n",
      "With a large amount of annotated data, supervised methods\n",
      "achieve a great success in various tasks including object re-ID\n",
      "[11], [14], [27], [28], [29], [30], [31], [32], while maritime\n",
      "vessel re-ID still has a long way to go due mainly to the lack\n",
      "of large-scale annotated datasets.Meanwhile, in order to avoid\n",
      "the high cost of manual annotation, the re-ID community pays\n",
      "increasing attention to unsupervised methods.A. Unsupervised Person/Vehicle Re-ID\n",
      "Efforts have been made to develop unsupervised domain\n",
      "adaptation methods for person and vehicle re-ID, which trans-\n",
      "fer the learned knowledge from the labeled source domain to\n",
      "the unlabeled target domain [17], [33], [34], [35], [36], [37],\n",
      "[38], [39].However, due to the difficulty of data collection,\n",
      "there is no public dataset that can be used as the source domain\n",
      "data and thus such domain adaption methods can hardly be\n",
      "used for vessel re-ID.\n",
      "====================\n",
      "翻譯結果:\n",
      " 廣泛的實驗表明，所提出的MCL方法在無監督船舶重識別方面，表現優於現有的最先進方法。相較於有大量注釋數據的監督方法，無監督方法常常因缺乏大規模標註數據集而受限，導致海上船舶重識別的成效較差。因此，為了避免昂貴的手動標注成本，本領域越來越關注無監督方法。先前已針對人和車輛重識別開展了無監督領域適應方法的研究，該方法將從標註源域學習到的知識轉移到未標註目標域中。但是，由於數據收集的困難度，目前尚無可用作源域數據的公共數據集，因此這些領域適應方法很難用於船舶重識別。\n",
      "====================\n",
      "====================\n",
      "原文:\n",
      " With only one labeled example for each\n",
      "identity, Wu et al.[40], [41] explored one-shot person re-ID,\n",
      "where pseudo-labels are gradually assigned to unlabeled data\n",
      "by the identity labels of their nearest labeled neighbors.Recently, some methods [15], [16], [18], [21], [22], [23],\n",
      "[42] addressed the problem of unsupervised person and vehicle\n",
      "re-ID by learning discriminative representations without any\n",
      "labeled samples.For example, BUC [15] and HTC [18]\n",
      "generated pseudo labels by hierarchical clustering methods,\n",
      "and updated models with the pseudo labels using classifica-\n",
      "tion or triplet loss.Lin et al.[23] exploited the cross-camera\n",
      "similarity within the same identity from both the original\n",
      "and the style-transferred training images, where the pseudo\n",
      "labels were also generated by bottom-up clustering.To avoid\n",
      "the effect of noisy pseudo labels, MMCL [16] formulated\n",
      "unsupervised person re-ID as a multi-label classification prob-\n",
      "lem by maintaining a memory bank of all instances in the\n",
      "dataset.Similar to MMCL, SSML [22] conducted positive\n",
      "label mining by a joint metric learning to obtain multi-labels\n",
      "for unsupervised vehicle re-ID.\n",
      "====================\n",
      "翻譯結果:\n",
      " Wu等人[40]，[41]只有每個身份的一個已標記範例，探索了一次性人物重新識別，其中偽標籤逐漸分配給未標記數據，通過其最接近的已標記鄰居的身份標籤。最近，一些方法[15]，[16]，[18]，[21]，[22]，[23]通過學習有判別力的表示，在沒有任何標記樣本的情況下解決了無監督的人物和車輛重新識別問題。例如，BUC [15]和HTC [18]通過層次聚類方法生成偽標籤，並使用分類或三元組損失使用偽標籤更新模型。林等人[23]利用來自原始和風格轉換訓練圖像的相同身份之間的跨攝像頭相似性，其中假標籤也是通過自下而上的聚類生成的。為了避免噪聲假標籤的影響，MMCL [16]通過維護數據集中所有實例的記憶銀行，將無監督的人物重新識別形式化為多標籤分類問題。與MMCL類似，SSML [22]通過聯合度量學習進行正標籤挖掘，以獲取無監督車輛重新識別的多標籤。\n",
      "====================\n",
      "====================\n",
      "原文:\n",
      " There are also some recent\n",
      "works attempting to improve the quality of pseudo labels [43],\n",
      "[44], [45], [46] or involve extra camera information [47], [48]\n",
      "for better unsupervised person re-ID, which do not align with\n",
      "the focus of this paper.Although the aforementioned methods achieved promising\n",
      "results on person/vehicle re-ID, the direct application of most\n",
      "of these methods to vessel re-ID does not produce good results\n",
      "as it is more challenging than other re-ID tasks due to the\n",
      "nature of vessel images.B.Maritime Vessel Re-ID\n",
      "Initial attempts have been made in the field of maritime\n",
      "vessel re-ID over the past several years [1], [2], [3], [4],\n",
      "[5], [49], [50], [51], [52].The first vessel re-ID dataset\n",
      "that consists of 5, 523 vessel images was introduced in [1]\n",
      "while it is not sufficiently large for deep learning methods.This article has been accepted for inclusion in a future issue of this journal.Content is final as presented, with the exception of pagination.Authorized licensed use limited to: National Chung Hsing Univ.. Downloaded on March 27,2023 at 07:07:45 UTC from IEEE Xplore.\n",
      "====================\n",
      "翻譯結果:\n",
      " 也有一些最近的研究試圖改善偽標籤的質量[43]，[44]，[45]，[46]，或者涉及額外的相機信息[47]，[48]，以實現更好的無監督人物重新識別，但這些研究與本文的重點不符。盡管上述方法在人/車身份識別方面取得了有希望的結果，但是大多數這些方法的直接應用對於船舶重新識別並不會產生良好的結果，因為由於船舶圖像的特殊性質，這一任務比其他重新識別任務更具挑戰性。\n",
      "\n",
      "B.船舶重新識別\n",
      "近幾年，人們在船舶重新識別領域進行了一些初步的嘗試[1]，[2]，[3]，[4]，[5]，[49]，[50]，[51]，[52]。第一個包含5,523張船舶圖像的船舶重新識別數據集在[1]中介紹，但對於深度學習方法來說，該數據集並不足夠大。本文已被接受包含在未來的一期期刊中。內容在呈現時是最終版本，但有時會更改頁碼。使用許可僅限於：國立中興大學。從IEEE Xplore於2023年3月27日07:07:45 UTC下載。\n",
      "====================\n",
      "====================\n",
      "原文:\n",
      " Restrictions apply.\n",
      "====================\n",
      "翻譯結果:\n",
      " 有所限制。\n",
      "====================\n",
      "====================\n",
      "原文:\n",
      " ZHANG et al.: UNSUPERVISED MARITIME VESSEL RE-IDENTIFICATION WITH MCL 3\n",
      "VesselID-539 [3], the first large-scale dataset for vessel re-ID,\n",
      "is composed of 149, 363 images of 539 vessels.Unfortu-\n",
      "nately, it has not been made publicly available so far.More\n",
      "recently, Ghahremani et al.[4] presented the VR-VCA dataset\n",
      "that includes 4614 images of 729 vessels for vessel re-ID.However, this dataset cannot be downloaded from the website\n",
      "provided in their paper.Most of existing methods [1], [2], [3], [4], [5], [51],\n",
      "[52] for vessel re-ID learn discriminative CNN features\n",
      "via triplet and identity losses based on cross entropy.For instance, Ghahremani et al.[2] constructed an identity-\n",
      "oriented re-ID network for vessel re-ID, which improved\n",
      "the triplet method with an identity-oriented loss.Qiao et al.[3] proposed well-designed loss functions and explored the\n",
      "features of discriminative local parts along with the commonly\n",
      "used global features of vessel images for better representations.Besides, Groot et al.\n",
      "====================\n",
      "翻譯結果:\n",
      " 等人：使用MCL 3來進行無監督海上船舶重新識別\n",
      "VesselID-539 [3]是第一個大規模的船舶重新識別數據集，由539艘船舶的149,363張圖像組成。不幸的是，迄今為止它還未公開發佈。最近，Ghahremani等人[4]提出了VR-VCA數據集，包括729艘船舶的4614張圖像，用於船舶重新識別。然而，無法從他們論文中提供的網站上下載此數據集。現有的大多數船舶重新識別方法[1]，[2]，[3]，[4]，[5]，[51]，[52]通過基於交叉熵的三元組和身份損失學習具有區分性的CNN特徵。例如，Ghahremani等人[2]為船舶重新識別構建了一個面向身份的重新識別網絡，改進了三元組方法，並引入了身份導向損失。Qiao等人[3]提出了設計良好的損失函數，並探索了特徵區分性較好的局部部位特徵以及通常用於船舶圖像的全局特徵，以提高表示能力。此外，Groot等人......\n",
      "====================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "原文:\n",
      " [5], [51], [52] built a tracklet-based\n",
      "matching component to re-identify vessels across two cameras.Han et al.[50] made the first attempt to train a deep neural\n",
      "network capable of retrieving images of a given vessel by\n",
      "contrastive learning where the generated pseudo labels were\n",
      "refined according to average similarity between samples and\n",
      "camera-aware prototypes.However, there exists no work that takes into account the\n",
      "characteristics of vessel images when performing unsupervised\n",
      "vessel re-ID.Different from previous methods, the proposed\n",
      "MCL aims to deal with the HPP caused specifically by vessel\n",
      "re-ID when using conventional contrastive learning.C. Contrastive Learning\n",
      "Contrastive methods [53], [54], [55] have been successful\n",
      "in self-supervised representation learning.These methods learn\n",
      "instance-level discriminative representation by contrasting pos-\n",
      "itive pairs against negative pairs without supervisory signals.In addition to this, contrastive learning has been widely used\n",
      "in many computer vision tasks.\n",
      "====================\n",
      "翻譯結果:\n",
      " [5]、[51]、[52]建立了一個基於軌跡的匹配組件，\n",
      "以在兩個攝像機之間重新識別船隻。韓等人[50]嘗試訓練一個深度神經網絡，能夠通過對比學習檢索給定船隻的圖像，其中生成的偽標籤根據樣本和相機感知的原型之間的平均相似度進行了優化。然而，在執行無監督船隻重新識別時，目前沒有考慮到船隻圖像的特性的方法。與以前的方法不同，所提出的MCL旨在處理使用常規對比學習時船隻重新識別特有的HPP問題。C.對比學習\n",
      "對比方法[53]、[54]、[55]在自我監督表示學習中取得了成功。這些方法通過將正樣本與負樣本進行對比來學習實例級別的判別表示，而沒有監督信號。除此之外，對比學習已被廣泛應用於許多計算機視覺任務中。\n",
      "====================\n",
      "====================\n",
      "原文:\n",
      " For example, Wu et al.[56] extended the concept of contrast to cross-modal feature\n",
      "matching to solve the audio-visual event localization problem.Efforts have been made to introduce the concept of con-\n",
      "trastive learning into unsupervised re-ID [16], [17], [19], [34].The highly cited SpCL [17] conducted contrastive learning\n",
      "with a dynamic hybrid memory and MMCL [16] was a variant\n",
      "of contrastive learning based on a different loss function.They\n",
      "implicitly coped with the HPP caused by instance-level con-\n",
      "trastive learning by using well-designed sampling strategies\n",
      "or complicated augmentation settings, which prevented the\n",
      "representation space from degenerating to an uniform feature\n",
      "distribution.ICE [19] used views augmented differently from\n",
      "an image as positive pairs and performed cluster-level and\n",
      "instance-level contrastive learning simultaneously for better\n",
      "performance.Although the concept of multi-level contrast has been\n",
      "widely used in many other fields such as visual object clas-\n",
      "sification [57], [58], natural language processing [59], [60],\n",
      "and graph representation learning [61], the implementations\n",
      "of multi-level contrastive learning vary considerably across\n",
      "application scenarios.\n",
      "====================\n",
      "翻譯結果:\n",
      " 例如，吳等人[56]將對比概念擴展到跨模態特徵匹配中，以解決視聽事件定位問題。在無監督的再識別[16]、[17]、[19]、[34]中，人們努力將對比學習引入其中。被引用次數很高的SpCL [17]通過具有動態混合記憶的對比學習進行，而MMCL [16]則是基於不同損失函數的對比學習變體。它們通過使用精心設計的採樣策略或複雜的增強設置來隱含地應對實例級對比學習所引起的HPP，從而防止表示空間退化為均勻特徵分布。ICE [19]使用與圖像不同的視圖作為正對，同時進行群集級和實例級對比學習，以提高性能。盡管多層對比的概念在許多其他領域（如視覺對象分類[57]、[58]、自然語言處理[59]、[60]和圖形表示學習[61]）中被廣泛使用，但不同應用情況下實現多層對比學習的方式各不相同。\n",
      "====================\n",
      "====================\n",
      "原文:\n",
      " As far as we know, ICE [19] is the firstand only multi-level contrastive framework for object re-ID\n",
      "before the proposed MCL, which differs from ICE mainly in\n",
      "two points.First, ICE only considered the hardest positive\n",
      "samples when computing the instance-level contrastive loss,\n",
      "while our MCL takes all samples in an intra-batch cluster\n",
      "as positives to more sufficiently make use of the information\n",
      "derived from the training set and solve the HPP better.Second,\n",
      "MCL does not need a memory bank of the whole dataset\n",
      "during the training, which makes it possible to train re-ID\n",
      "models on large-scale datasets.III.M ETHOD\n",
      "In this paper, we propose a multi-level contrastive learn-\n",
      "ing (MCL) framework for unsupervised vessel re-ID.Given\n",
      "an unlabeled dataset X, our goal is to train a powerful vessel\n",
      "re-ID model on it.A. Overview of MCL\n",
      "As illustrated in Fig.2, our MCL algorithm consists of two\n",
      "alternating phases: pseudo-label generation and multi-level\n",
      "contrastive learning.Before the t-th training epoch, the fea-\n",
      "tures of all vessel images in Xare extracted by the encoder\n",
      "ft−1\n",
      "θtrained in previous epoch, and clustered to generate the\n",
      "pseudo label for each vessel image.\n",
      "====================\n",
      "翻譯結果:\n",
      " 據我們所知，ICE [19] 是在提出 MCL 之前，第一個也是唯一一個針對物體重新識別的多層對比框架。然而，MCL 與 ICE 有兩個不同之處。首先，當計算實例級對比損失時，ICE 只考慮最難的正樣本，而我們的 MCL 則將批內聚類中的所有樣本作為正樣本，以更充分地利用訓練集中的信息並更好地解決 HPP 類型問題。其次，MCL 在訓練過程中不需要整個數據集的記憶庫，這使得在大規模數據集上訓練重新識別模型成為可能。\n",
      "\n",
      "III.方法\n",
      "本文中，我們提出了一個多層對比學習 (MCL) 框架，用於無監督船舶重新識別。給定一個未標記的數據集 X，我們的目標是在其中訓練一個強大的船舶重新識別模型。 \n",
      "\n",
      "A. MCL 概述\n",
      "如圖2所示，我們的 MCL 算法由兩個交替的階段組成：偽標籤生成和多層對比學習。在第 t 個訓練時期之前，先前訓練的編碼器 ft-1θ 提取了 X 中所有船舶圖像的特徵，並將其進行聚類以為每個船舶圖像生成偽標籤。\n",
      "====================\n",
      "====================\n",
      "原文:\n",
      " Then, in each iteration of\n",
      "training, a sampled vessel image xkis randomly augmented\n",
      "by the data augmentation module Ato two views, noted as\n",
      "˜x2k−1and˜x2k, which the MCL framework takes as input for\n",
      "further contrastive learning.During the multi-level contrastive\n",
      "learning, these two augmented views are processed by the\n",
      "current encoder ft\n",
      "θto obtain the feature vectors z2k−1=\n",
      "ft\n",
      "θ(˜x2k−1)andz2k=ft\n",
      "θ(˜x2k).After that, on one hand, we use\n",
      "such feature vectors to compute an instance-level contrastive\n",
      "loss formed as a variant of the classical InfoNCE loss [62].This loss function takes all samples with the same pseudo\n",
      "label in the current mini-batch as positives to mitigate the\n",
      "HPP for unsupervised vessel re-ID (with details in Section III-\n",
      "B).On the other hand, the intra-batch centroids are further\n",
      "obtained based on the feature vectors of the samples in\n",
      "current mini-batch.Thus, MCL benefits from the cluster-level\n",
      "contrastive discrimination for solving the HPP (with details\n",
      "in Section III-C).As a result, the vessel re-ID model, i.e.\n",
      "====================\n",
      "翻譯結果:\n",
      " 在每次訓練的迭代中，一個被抽樣的血管影像xk會以資料增強模組A的方式隨機產生兩個視圖，標記為˜x2k−1和˜x2k，這兩個視圖為MCL框架的輸入。在多級對比學習期間，這兩個增強的視圖被當前的編碼器ft\n",
      "θ處理，以獲取特徵向量z2k−1=ft\n",
      "θ(˜x2k−1)和z2k=ft\n",
      "θ(˜x2k)。然後，一方面，我們使用這些特徵向量計算一個實例級對比損失，形成經典InfoNCE損失的變體[62]。此損失函數將當前小批次中具有相同偽標籤的所有樣本視為正樣本，以緩解無監督血管再識別中的HPP（有關細節請參見III-B章節）。另一方面，內批次中心點是基於當前小批次中樣本的特徵向量進一步獲取的。因此，MCL從叢集級對比區分中受益，以解決HPP（有關詳細信息，請參見第III-C章節）。因此，血管再識別模型，即\n",
      "====================\n",
      "====================\n",
      "原文:\n",
      " the\n",
      "encoder ft\n",
      "θ, is optimised by both instance-level and cluster-\n",
      "level contrastive learning.B. Instance-Level Discrimination\n",
      "Instance-level contrastive learning (ICL) means that the\n",
      "encoder is optimised by contrasting positive and negative pairs\n",
      "composed of two independent vessel samples.In the proposed\n",
      "MCL, ICL is implemented with augmented vessel images\n",
      "˜x2k−1and˜x2k, rather than their corresponding original vessel\n",
      "image xk.As a result, there are a total of 2 Nvessel samples\n",
      "in a mini-batch of size N. Once the features of the aug-\n",
      "mented vessel images have been extracted, the instance-level\n",
      "contrastive loss can be calculated using a modified version ofThis article has been accepted for inclusion in a future issue of this journal.Content is final as presented, with the exception of pagination.Authorized licensed use limited to: National Chung Hsing Univ.. Downloaded on March 27,2023 at 07:07:45 UTC from IEEE Xplore.Restrictions apply.\n",
      "====================\n",
      "翻譯結果:\n",
      " 編碼器ft θ 通過個體水平和群集水平對比學習進行優化。B.個體水平區分\n",
      "個體水平對比學習 (ICL) 意味著編碼器通過比較由兩個獨立的血管樣本組成的正負對進行優化。在所提出的 MCL 中，ICL 使用增強的血管影像˜x 2k-1和˜x 2k 進行實現，而不是它們對應的原始血管影像 xk。因此，在大小為 N 的 mini-batch 中有總共 2N 個血管樣本。一旦提取了增強血管影像的特徵，個體水平對比損失可以使用修改版來計算。本文已被接受以包含在本期期刊中。內容已最終呈現，但頁碼除外。授權使用限於:國立中興大學。2023 年 3 月 27 日從 IEEE Xplore 下載，限制適用。\n",
      "====================\n",
      "====================\n",
      "原文:\n",
      " 4 IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS\n",
      "Fig.2.Architecture of the MCL for unsupervised vessel re-ID.The MCL algorithm consists of two phases, i.e.pseudo-label generation and multi-level\n",
      "contrastive learning, which are indicated by green and blue channels respectively.The green and red ellipses encircle the positive and all negative samples\n",
      "for an anchor sample respectively in the contrastive learning.the InfoNCE loss, expressed as\n",
      "LIC L(i)=2N/summationdisplay\n",
      "j=11[j̸=i∧yj=yi]L(i,j), (1)\n",
      "L(i,j)= − logexp(s i,j/τ1)\n",
      "/summationtext2N\n",
      "k=11[k̸=i∧(k=j∨yk̸=yi)]exp(s i,k/τ1),\n",
      "(2)\n",
      "si,j=zi·zj\n",
      "∥zi∥2∥zj∥2, (3)\n",
      "where ∥ · ∥2is the ℓ2-norm, ziandzjdenote the features of\n",
      "thei-th and the j-th augmented vessel images respectively,\n",
      "thus si,jis the similarity between these two images.Nis the\n",
      "size of a mini-batch, and τ1is a temperature parameter.The\n",
      "total loss is averaged over all views.After the clustering operation of a pseudo-label generator G\n",
      "(see Section V-A.2 for details), different images in the same\n",
      "cluster correspond to multiple instances of the same vessel\n",
      "identity in vessel re-ID.\n",
      "====================\n",
      "翻譯結果:\n",
      " 第四篇《智能交通系統IEEE交易》\n",
      "Fig.2.无监督船只重识别MCL的架构。MCL算法由伪标签生成和多级对比学习两个阶段组成，分别用绿色和蓝色通道表示。对于对比学习中的锚点样本，绿色和红色椭圆分别包括正样本和所有负样本。损失函数InfoNCE如下表述：\n",
      "LIC L(i)=2N/summationdisplay\n",
      "j=11[j̸=i∧yj=yi]L(i,j), (1)\n",
      "L(i,j)= − logexp(s i,j/τ1)\n",
      "/summationtext2N\n",
      "k=11[k̸=i∧(k=j∨yk̸=yi)]exp(s i,k/τ1),\n",
      "(2)\n",
      "si,j=zi·zj\n",
      "∥zi∥2∥zj∥2, (3)\n",
      "其中，∥ · ∥2是ℓ2-norm，zi和zj分别表示第i个和第j个增强船只图像的特征，因此si,j是这两个图像之间的相似度。N是小批量大小，τ1是温度参数。总损失平均分配在所有视图上。在伪标签生成器G的聚类操作之后（详见第V-A.2节），同一簇中的不同图像对应于船只重识别中同一船只身份的多个实例。\n",
      "====================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "原文:\n",
      " As such, the sampled images with\n",
      "the same pseudo label should be regarded as positives and\n",
      "should not be pushed away during the training.Thus, for\n",
      "thei-th augmented vessel image, also known as the anchor\n",
      "sample in Fig.2, the ICL excludes potential positives that\n",
      "share the same pseudo label with it from the denominator of\n",
      "Equ.2.By optimising the encoder with LIC L(i), the positives\n",
      "are pull towards from the anchor sample, which effectively\n",
      "solves the HPP.Note that different from most of the previous\n",
      "unsupervised re-ID methods [15], [17], [19], [21], [25], [37],we employ data augmentation as an extra mean to construct\n",
      "positive pairs.Specifically, for a vessel image xkin the mini-\n",
      "batch, its augmented views, i.e.˜x2k−1and˜x2k, form a positive\n",
      "pair for ICL as in conventional self-supervised representa-\n",
      "tion learning [54].Therefore, ˜x2k−1and˜x2kshare the same\n",
      "pseudo-label.IfGoutputs the index of an image in the dataset as its label\n",
      "(i.e.each image represents an individual class), ICL degener-\n",
      "ates to the self-supervised learning framework SimCLR [54].\n",
      "====================\n",
      "翻譯結果:\n",
      " 因此，對於具有相同虛擬標籤的樣本圖像，應視為正樣本，在訓練期間不應被推開。因此，在圖2中稱為錨點樣本的第i個增強血管影像（也稱為第i個樣本），ICL會從Equ. 2的分母中排除與其共享相同虛擬標籤的潛在正樣本。藉由優化具有LIC L（i）的編碼器，正樣本從錨點樣本中被拉向，有效地解決了HPP。需要注意的是，與大多數以前的無監督重新識別方法[15]，[17]，[19]，[21]，[25]，[37]不同，我們採用數據增強作為一種額外的方法來構建正樣本對。具體而言，對於小批量的血管影像xk，它的增強視圖，即˜x2k-1和˜x2k，形成ICL的正對，就像傳統的自監督表示學習中一樣。因此，˜x2k-1和˜x2k共享相同的虛擬標籤。如果G輸出數據集中圖像的索引作為其標籤（即每個圖像代表一個獨立的類），ICL會退化為自監督學習框架SimCLR [54]。\n",
      "====================\n",
      "====================\n",
      "原文:\n",
      " Note that ICL is not identical to the supervised contrastive\n",
      "learning (SCL) [63] with pseudo labels, where all other\n",
      "positives are considered to be negatives when computing the\n",
      "loss for a specific positive pair.In terms of the hard instance\n",
      "contrast in ICE [19], where only the hardest positive sample\n",
      "is taken into consideration when computing the instance-level\n",
      "contrastive loss, the ability of the model to re-identify easy\n",
      "samples is weakened as most of the simple samples are left\n",
      "out from training.In order to involve more positive samples\n",
      "into training, the MCL takes all vessel samples with the\n",
      "same pseudo label as positives to obtain the instance-level\n",
      "contrastive loss and solve the HPP problem better.C. Intra-Batch Cluster-Level Discrimination\n",
      "We design an intra-batch cluster-level contrastive learn-\n",
      "ing (CCL) branch as a complement to ICL.Based on the intra-\n",
      "batch centroids, CCL also serves as a solution for the HPP.Different from the way to build positive pairs in ICL, CCLThis article has been accepted for inclusion in a future issue of this journal.\n",
      "====================\n",
      "翻譯結果:\n",
      " 請注意，ICL與擬似標籤的監督對比學習（SCL）[63]並不完全相同，在計算特定正樣本對的損失時，所有其他正樣本都被視為負樣本。就ICE [19]中的硬實例對比而言，在計算實例級對比損失時，只考慮最困難的正樣本，模型重新識別簡單樣本的能力會被削弱，因為大部分簡單樣本都沒有進行訓練。為了將更多正樣本納入訓練，MCL將所有與假標籤相同的管脈樣本作為正樣本進行實例級對比學習，更好地解決了HPP問題。C.批內群集級區分\n",
      "我們設計了一個批內群集級對比學習（CCL）分支，作為ICL的補充。基於批內中心，CCL也是HPP的一種解決方案。與ICL中建立正對比對的方式不同，CCL......\n",
      "====================\n",
      "====================\n",
      "原文:\n",
      " Content is final as presented, with the exception of pagination.Authorized licensed use limited to: National Chung Hsing Univ.. Downloaded on March 27,2023 at 07:07:45 UTC from IEEE Xplore.Restrictions apply.\n",
      "====================\n",
      "翻譯結果:\n",
      " 內容在排版以外已經被確認，但需要注意的是，內容的分頁可能會有所不同。\n",
      "授權許可使用僅限於：國立中興大學。本文檔於2023年3月27日07:07:45 UTC從IEEE Xplore下載。限制條款適用。\n",
      "====================\n",
      "====================\n",
      "原文:\n",
      " ZHANG et al.: UNSUPERVISED MARITIME VESSEL RE-IDENTIFICATION WITH MCL 5\n",
      "forms a positive pair as an anchor sample and the centroid\n",
      "of the cluster it belongs to.Specifically, for a selected cluster,\n",
      "Ncvessel images are sampled to form a mini-batch, and the set\n",
      "of these Ncimages is known as the intra-batch cluster in this\n",
      "paper.As is described above, Ncoriginal vessel images are\n",
      "pre-processed to 2 Ncaugmented images before the contrastive\n",
      "learning.However, only Ncrather than 2 Ncaugmented images\n",
      "are used for CCL following ICE [19], which means that CCL\n",
      "takes into account only the first augmented view of each\n",
      "original vessel image.With features of these augmented vessel\n",
      "images, the centroid of an intra-batch cluster is calculated as\n",
      "φc=1\n",
      "NcNc/summationdisplay\n",
      "n=1zn, (4)\n",
      "where Ncis the number of vessel images sampled from a\n",
      "specific cluster of the training set, and zndenotes feature of\n",
      "then-th augmented image used for CCL.Therefore, for an anchor sample in the c-th intra-batch\n",
      "cluster, its potential positives that share the same pseudo label\n",
      "are partly embedded into the positive centroid φc, which is\n",
      "supposed to be pulled towards the anchor sample during the\n",
      "training.\n",
      "====================\n",
      "翻譯結果:\n",
      " ZHANG等人：使用MCL 5的非監督式海上船舶重新識別通過將錨定樣本和該叢集的中心點形成正對號，形成正對。具體而言，對於選定的叢集，Ncvessel圖像被採樣形成迷你批次，這些Nc圖像的集合在本文中被稱為內部批次簇。正如上面所述，Nc原始船舶圖像在對比學習之前被預處理為2Ncaugmented圖像。但是，只有Nc而不是2Ncaugmented圖像用於CCL，遵循ICE [19]的方法，這意味著CCL僅考慮每個原始船舶圖像的第一個擴增視圖。使用這些擴增船舶圖像的特徵，內部批次簇的中心點被計算為φc = 1 / Nc Nc / summationdisplay n = 1zn，其中Nc是從訓練集的特定叢集中採樣的船舶圖像數量，zn表示用於CCL的第n個擴增圖像的特徵。因此，對於內部批次簇中的錨定樣本，與其共享相同偽標籤的潛在正對被部分嵌入到正對中心φc中，在訓練期間應拉近該正對中心與錨定樣本的距離。\n",
      "====================\n",
      "====================\n",
      "原文:\n",
      " In this sense, CCL also helps to handle the HPP.Based on the centroids of intra-bath clusters, the cluster-level\n",
      "contrastive loss is computed as\n",
      "LCC L(i)= − logexp(s i,c/τ2)\n",
      "/summationtextC\n",
      "k=1exp(si,k/τ2), (5)\n",
      "si,k=zi·φk\n",
      "∥zi∥2∥φk∥2, (6)\n",
      "where si,kdenotes the similarity between the i-th augmented\n",
      "vessel image and the k-th intra-batch cluster centroid.Thus\n",
      "si,cdenotes the similarity between the i-th augmented image\n",
      "and its positive cluster centroid.τ2is a temperature parameter\n",
      "for cluster-level contrastive learning.Although this cluster-level loss looks exactly the same as\n",
      "the camera-agnostic proxy contrastive loss proposed in ICE\n",
      "[19], they are inherently different.ICE built a camera-agnostic\n",
      "memory bank for all cluster centroids of the whole dataset,\n",
      "which allows to draw closer the positive centroid of the anchor\n",
      "sample while pushing away all of its negative centroids in\n",
      "the memory bank in every iteration.Note that the memory\n",
      "bank is a memory-consuming setting particularly when facing\n",
      "a large-scale dataset.\n",
      "====================\n",
      "翻譯結果:\n",
      " 在這個意義上，CCL還有助於處理HPP。基於 intra-bath cluster 的質心，計算聚類級對比損失如下:\n",
      "LCC L(i)= − logexp(s i,c/τ2)\n",
      "/summationtextC\n",
      "k=1exp(si,k/τ2), (5)\n",
      "si,k=zi·φk\n",
      "∥zi∥2∥φk∥2, (6)\n",
      "其中，si,k 表示第 i 個擴增的血管影像和第 k 個 intra-batch 聚類中心之間的相似度。因此，si,c 表示第 i 個擴增的影像和其正聚類質心之間的相似度。τ2 是聚類級對比學習的溫度參數。儘管這個聚類級損失看起來與 ICE[19] 提出的面向攝像機的代理對比損失完全相同，但它們在本質上是不同的。ICE 為整個數據集的所有聚類質心建立了一個面向攝像機的記憶庫，這允許在每次迭代中拉近錨樣本的正聚類質心，同時推開記憶庫中所有負聚類質心。請注意，當面對大規模數據集時，記憶庫是一種佔用記憶體的設置。\n",
      "====================\n",
      "====================\n",
      "原文:\n",
      " In contrast, CCL does not rely on any\n",
      "memory bank.It computes the cluster centroids in an intra-\n",
      "batch manner, and thus the positive and negative centroids are\n",
      "all from the sampled batch, leading to a far smaller number\n",
      "of negative centroids.D. Total Loss for Training MCL\n",
      "Although either ICL or CCL mitigates the negative effects of\n",
      "the HPP individually, they are complementary to some extent.On the one hand, ICL makes it possible to learn the detailed\n",
      "differences between vessel samples.On the other hand, since\n",
      "the centroid calculated in Equ.4is a combination of all\n",
      "representative negative samples in an intra-batch cluster, CCL\n",
      "is more robust to noisy vessel samples with incorrect pseudo\n",
      "labels.To generate a better vessel re-ID model, we conductthe contrastive representation learning in both instance-level\n",
      "and cluster-level through a joint training.The total loss is a\n",
      "weighted sum of LIC Lin Equ.1andLCC L in Equ.5:\n",
      "Ltotal=1\n",
      "2N2N/summationdisplay\n",
      "i=1LIC L(i)+λN/summationdisplay\n",
      "i=1LCC L(i), (7)\n",
      "where λis a weighted parameter to balance the instance-level\n",
      "and the cluster-level contrastive losses, Nis the size of a\n",
      "mini-batch.\n",
      "====================\n",
      "翻譯結果:\n",
      " 相比之下，CCL不依賴任何記憶銀行。它以一種組內批次方式計算群集中心，因此正面和負面中心都來自樣本批次，從而導致負面中心數量大大減少。D. MCL訓練的總損失\n",
      "\n",
      "儘管ICL或CCL逐個緩解了HPP的負面影響，但它們在某種程度上是互補的。一方面，ICL使得學習船舶樣本之間的詳細差異成為可能。另一方面，由於在Equ.4中計算的中心點是組內集群中所有代表性負樣本的組合，因此對於具有不正確偽標籤的有噪聲船舶樣本，CCL更加強韌。為了生成更好的船舶重新識別模型，我們通過聯合訓練在實例級和群集級別進行對比表示學習。總損失是LIC Lin Equ.1和LCC L in Equ.5的加權和：\n",
      "\n",
      "Ltotal=1\n",
      "2N2N/summationdisplay\n",
      "i=1LIC L(i)+λN/summationdisplay\n",
      "i=1LCC L(i)，(7)\n",
      "\n",
      "其中λ是平衡實例級和群集級對比損失的加權參數，N是一個小批次的大小。\n",
      "====================\n",
      "====================\n",
      "原文:\n",
      " Algorithm 1 MCL for Vessel Re-ID\n",
      "Require: a set Xof unlabeled vessel images, the num-\n",
      "ber of training epochs T, the number of iterations per\n",
      "epoch I, batch size N, data augmentation module A,\n",
      "encoder network fθ, pseudo-label generator G, weighting\n",
      "factor λ.1:fort∈ {1, ..., T}do\n",
      "2: foreach vessel image xinXdo\n",
      "3: generate pseudo label y = G(ft−1\n",
      "θ(x))\n",
      "4: end for\n",
      "5: fori∈ {1, ..., I}do\n",
      "6: build a mini-batch {xk}N\n",
      "k=1fromXby randomly\n",
      "sampling Cvessel identities and N/Cvessel images\n",
      "per identity\n",
      "7: fork∈ {1, ..., N}do\n",
      "8: build the random augmentation operations a∼A\n",
      "to obtain the first augmented view\n",
      "9: ˜x2k−1=a(xk),z2k−1=ft\n",
      "θ(˜x2k−1),y2k−1=yk\n",
      "10: build the random augmentation operations a′∼A\n",
      "to obtain the second augmented view\n",
      "11: ˜x2k=a′(xk),z2k=ft\n",
      "θ(˜x2k),y2k=yk\n",
      "12: end for\n",
      "13: fori∈ {1, ..., 2N}and j∈ {1, ..., 2N}do\n",
      "14: compute si,jby Equ.3\n",
      "15: end for\n",
      "16: fori∈ {1, ..., 2N}do\n",
      "17: compute LIC L(i)by Equ.2and Equ.1\n",
      "18: end for\n",
      "19: LIC L=1\n",
      "2N/summationtext2N\n",
      "i=1LIC L(i)\n",
      "20: fori∈ {1, ..., N}andk∈ {1, .\n",
      "====================\n",
      "翻譯結果:\n",
      " , ., 2}do\n",
      "21: compute softmax score pi,kaccording to Equ.4\n",
      "22: end for\n",
      "23: fori∈ {1, ..., N}do\n",
      "24: compute cross-entropy loss Li,ceby Equ.5\n",
      "25: compute triplet loss Li,triwith Equs.6and\n",
      "7\n",
      "26: compute final loss Li=L(i)ce+λLi,tri\n",
      "27: end for\n",
      "28: update fθby back-propagating gradients from\n",
      "mini-batch loss L=1/NsummationiLi\n",
      "29: end for\n",
      "30:return fθas the trained encoder network\n",
      "\n",
      "\n",
      "算法1：用於船舶重識別的MCL\n",
      "要求：一組未標記的船舶圖像集X，訓練時期數T，每個時期的迭代次數I，批量大小N，數據增強模塊A，編碼器網絡fθ，假標籤生成器G，加權因子λ。\n",
      "1：對於t∈ {1，…，T}執行\n",
      "2：對於X中的每個船舶圖像xin執行以下操作\n",
      "3：生成假標籤y = G（ft−1θ（x））\n",
      "4：結束for循環\n",
      "5：對於i∈ {1，…，I}執行以下操作\n",
      "6：從X中隨機抽取C船舶身份和每個身份的N / C船舶圖像構建小批量{xk}N\n",
      "7：對於k∈ {1，…，N}執行以下操作\n",
      "8：構建隨機增強操作a∼A以獲取第一個增強視圖\n",
      "9：˜x2k−1 = a（xk），z2k−1 = ft\n",
      "θ（˜x2k−1），y2k−1 = yk\n",
      "10：構建隨機增強操作a′∼A以獲取第二個增強視圖\n",
      "11：˜x2k = a'（xk），z2k = ft\n",
      "θ（˜x2k），y2k = yk\n",
      "12：結束for循環\n",
      "13：對於i∈ {1，…，2N}和j∈ {1，…，2N}執行以下操作\n",
      "14：通過Equ.3計算si，j\n",
      "15：結束for循環\n",
      "16：對於i∈ {1，…，2N}執行以下操作\n",
      "17：通過Equ.2和Equ.1計算LIC L（i）\n",
      "18：結束for循環\n",
      "19：LIC L = 1 /\n",
      "2N / summationtext2N\n",
      "i = 1LIC L（i）\n",
      "20：對於i∈ {1，…，N}和k∈ {1，。、2}執行以下操作\n",
      "21：根據Equ.4計算softmax得分pi，k\n",
      "22：結束for循環\n",
      "23：對於i∈ {1，…，N}執行以下操作\n",
      "24：通過Equ.5計算交叉熵損失Li，ce\n",
      "25：使用Equs.6和7計算三元損失Li，tri\n",
      "26：計算最終損失Li = L（i）ce + λLi，tri\n",
      "27：結束for循環\n",
      "28：通過向小批量損失L = 1 / N summationi Li反向傳播梯度來更新fθ\n",
      "29：結束for循環\n",
      "30：將fθ作為經過訓練的編碼器網絡返回。\n",
      "====================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "原文:\n",
      " .., C}do\n",
      "21: compute si,kby Equ.4and Equ.6\n",
      "22: end for\n",
      "23: fori∈ {1, ..., N}do\n",
      "24: compute LCC L(i)by Equ.5\n",
      "25: end for\n",
      "26: LCC L=/summationtextN\n",
      "i=1LCC L(i)\n",
      "27: Ltotal=LIC L+λLCC L\n",
      "28: update encoder ft\n",
      "θto minimise Ltotal\n",
      "29: end for\n",
      "30:end for\n",
      "Ensure: encoder network fT\n",
      "θ⇒fθ\n",
      "As shown in Equ.7, we perform the mean and the sum\n",
      "operations for LIC L(i)andLCC L(i)respectively.Such a\n",
      "specific setting is based on the finding that the value of\n",
      "LCC L(i)is much smaller than that of LIC L(i).This is because\n",
      "1) the cluster centroid obtained by averaging the samplesThis article has been accepted for inclusion in a future issue of this journal.Content is final as presented, with the exception of pagination.Authorized licensed use limited to: National Chung Hsing Univ.. Downloaded on March 27,2023 at 07:07:45 UTC from IEEE Xplore.Restrictions apply.\n",
      "====================\n",
      "翻譯結果:\n",
      " ..，C}做\n",
      "21：藉由公式4和公式6計算si，k\n",
      "22：結束for迴圈\n",
      "23：對於i∈{1，...，N}做\n",
      "24：藉由公式5計算LCC L(i)\n",
      "25：結束for迴圈\n",
      "26：LCC L = /summationtextN i=1 LCC L(i)\n",
      "27：Ltotal = LIC L + λLCC L\n",
      "28：更新編碼器ft θ以最小化Ltotal\n",
      "29：結束for迴圈\n",
      "30：結束for迴圈\n",
      "確保：編碼器網路fT θ⇒fθ\n",
      "如公式7所示，我們分別對LIC L(i)和LCC L(i)進行平均和求和運算。這種特定設置基於一個發現，即LCC L(i)的值遠小於LIC L(i)的值。這是因為1）通過平均樣本得到的群集質心，\n",
      "====================\n",
      "====================\n",
      "原文:\n",
      " 6 IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS\n",
      "Fig.3.Images of various vessels in the proposed VesselReID dataset.Each column shows images of a selected vessel identity.with the same pseudo label weakens the differences between\n",
      "positive pairs, and 2) the number of negatives in the intra-batch\n",
      "CCL is far less than that in ICL.Besides, LCC L(i)is only\n",
      "computed with one augmented view of each vessel sample as\n",
      "LIC L(i)already poses a constraint to pull the two augmented\n",
      "views of each vessel sample close to each other.Thus in\n",
      "Equ.7, LCC L(i)is summed over Nsamples, which leads to a\n",
      "more efficient computation compared to the summation over\n",
      "2Nsamples and does not affect the final re-ID performance\n",
      "according to our experimental results.In this paper, λis set\n",
      "to 1, and the temperature parameters in Equ.2and Equ.5are\n",
      "both set to 0.05.The entire method of MCL is summarised in Alg.1.IV.V ESSEL RE-IDENTIFICATION DATASET\n",
      "Large-scale annotated dataset is considered to be an impor-\n",
      "tant premise for deep learning-based methods in computer\n",
      "vision.\n",
      "====================\n",
      "翻譯結果:\n",
      " 在IEEE Transactions on Intelligent Transportation Systems的第6篇文章中，圖3展示了提出的VesselReID數據集中各種船舶的圖像。每列顯示了所選船舶身份的圖像。假標籤被分配給相同的對正樣本會削弱正樣本之間的差異，而批內CCL中負樣本的數量遠少於ICL中的負樣本數量。此外，LCC L(i)僅使用每艘船樣本的一個增強視圖進行計算，因為LIC L(i)已經對將每艘船樣本的兩個增強視圖拉近到彼此附近施加了約束。因此，在Equ.7中，LCC L(i)在N樣本上求和，這導致與在2N樣本上求和相比，更有效地計算，並且不會影響最終的重新識別性能，根據我們的實驗結果。在本文中，λ被設置為1，Equ.2和Equ.5中的溫度參數都設置為0.05。MCL的整個方法在Alg.1中總結。IV.船舶重新識別數據集大規模帶標籤的數據集被認為是計算機視覺中基於深度學習方法的一個重要前提。\n",
      "====================\n",
      "====================\n",
      "原文:\n",
      " There already exist three re-ID datasets for maritime\n",
      "vessels [1], [3], [4], but they are either of small scale or not\n",
      "publicly available so far.In order to train powerful deep mod-\n",
      "els for vessel re-ID, we collected and annotated a large-scale\n",
      "vessel re-ID dataset, namely VesselReID, as a contribution of\n",
      "this paper.1In this section, we shall describe in detail the\n",
      "process of building the VesselReID dataset.A.Data Collection\n",
      "Due to the unique nature of maritime scenes, collecting ves-\n",
      "sel images through on-ground photography is labour-intensive\n",
      "and poses certain safety risks.Thus, we downloaded images of\n",
      "various vessels from some websites such as Shipspotting.2And\n",
      "we use the International Maritime Organisation (IMO) number\n",
      "attached to each vessel image as the vessel identity since it\n",
      "is unique for each vessel.We first downloaded images of the\n",
      "vessels with the IMO number appearing more than 150 times,\n",
      "which means that only if a vessel has at least 150 instances\n",
      "in the raw dataset, it will be recognised as an identity in\n",
      "our dataset.\n",
      "====================\n",
      "翻譯結果:\n",
      " 目前已經存在三個海洋船舶重識別（re-ID）數據集 [1]、[3]、[4]，但它們規模較小或尚未公開發布。為了訓練功能強大的深度模型用於船舶重識別，我們收集並標註了一個大規模的船舶重識別數據集，名為VesselReID，作為本文的貢獻之一。在本節中，我們將詳細描述構建VesselReID數據集的過程。\n",
      "\n",
      "A.數據收集\n",
      "由於海洋場景的獨特性，通過地面攝影收集船舶圖像需要耗費大量人力和存在一定的安全風險。因此，我們從一些網站（例如Shipspotting）下載了各種船舶的圖像，並使用附加在每個船舶圖像上的國際海事組織（IMO）編號作為船舶身份，因為每艘船的IMO編號都是獨特的。我們首先下載了IMO編號出現150次以上的船舶的圖像，這意味著只有在原始數據集中至少有150個實例的船舶才能被識別為數據集中的身份。\n",
      "====================\n",
      "====================\n",
      "原文:\n",
      " As such, we collected 361, 760 images belonging\n",
      "1The dataset can be obtained from https://github.com/vsislab/VesselReID\n",
      "2http://www.shipspotting.comto 1, 540 vessel identities in total.These images were then\n",
      "classified according to where they were taken.We selected\n",
      "6 locations where most vessel images were captured, and\n",
      "filtered out the images of vessels that only appeared at one\n",
      "location.Finally, the constructed VesselReID dataset consists of\n",
      "30,587 images of 1, 248 vessels, which means that there\n",
      "are approximately 25 images per vessel.Each vessel in Ves-\n",
      "selReID has a number of images captured at various times,\n",
      "places, and viewpoints as well as under different weather\n",
      "conditions.Since such images were taken by photographers all\n",
      "around the world using various equipment, they do not share\n",
      "the same size.However, we found that the aspect ratios of the\n",
      "images are not highly different.Thus, as we shall describe in\n",
      "Section V-A.4, we resize all images to a fixed size without\n",
      "significant deformations in data augmentation.\n",
      "====================\n",
      "翻譯結果:\n",
      " 因此，我們收集了361,760張船舶圖像，涵蓋了1540艘船舶。這些圖像根據它們被拍攝的地點進行了分類。我們選擇了6個拍攝船舶圖像最多的地點，並過濾掉只出現在一個地點的船舶圖像。最終構建的VesselReID數據集包含了1,248艘船舶的30,587張圖像，這意味著每艘船約有25張圖像。VesselReID中的每艘船都有在不同時間、地點、視角和不同天氣條件下拍攝的多張圖像。由於這些圖像是由全球各地的攝影師使用不同的設備拍攝的，因此它們的大小並不相同。但是，我們發現這些圖像的長寬比並不差異很大。因此，正如我們將在第V-A.4節中所描述的那樣，在數據擴展過程中，我們將所有圖像調整為固定的尺寸，而不會產生顯著的變形。\n",
      "====================\n",
      "====================\n",
      "原文:\n",
      " Fig.3shows\n",
      "some examples of vessel images in the VesselReID dataset.B.Data Division\n",
      "Following the normal settings in most popular re-ID datasets\n",
      "[24], [25], we split the entire dataset into training and testing\n",
      "sets.We randomly select 624 vessel identities (half of the\n",
      "vessel identities) for training, while images of the other\n",
      "624 vessels are used for testing.Therefore, there is no val-\n",
      "idation set in the original VesselReID dataset, and the re-ID\n",
      "models are supposed to be evaluated on the test set during the\n",
      "training as has been done in previous works [17], [19], [20],\n",
      "[21], [45].Moreover, to facilitate a more comprehensive and\n",
      "reasonable assessment of re-ID models, we additionally divide\n",
      "the VesselReID dataset into training, validating and testing\n",
      "sets as an extra split, where there are 550 vessel identities in\n",
      "both training and testing sets while the remaining 148 vessel\n",
      "identities are utilised for validation.C. Comparison With Other Datasets\n",
      "Tab.Ishows the comparison between VesselReID and other\n",
      "re-ID datasets.\n",
      "====================\n",
      "翻譯結果:\n",
      " 圖3顯示了VesselReID數據集中船舶圖像的一些示例。B.數據劃分\n",
      "遵循大多數流行的Re-ID數據集的正常設置[24]、[25]，我們將整個數據集分為訓練集和測試集。我們隨機選擇624個船舶身份（一半的船舶身份）進行訓練，而其它624個船舶的圖像用於測試。因此，在原始的VesselReID數據集中沒有驗證集，並且Re-ID模型應該在訓練期間在測試集上進行評估，就像以前的工作[17]、[19]、[20]、[21]、[45]所做的那樣。此外，為了促進Re-ID模型的更全面和合理的評估，我們另外將VesselReID數據集劃分為訓練、驗證和測試三個子集，其中訓練集和測試集中各有550個船舶身份，剩下的148個船舶身份被用於驗證。C.與其他數據集的比較\n",
      "表格I顯示了VesselReID與其他Re-ID數據集的比較。\n",
      "====================\n",
      "====================\n",
      "原文:\n",
      " In terms of dataset size, VesselReID is close to Market1501\n",
      "[24], one of the most popular datasets for person re-ID.Both\n",
      "of the two datasets have the same number of cameras orThis article has been accepted for inclusion in a future issue of this journal.Content is final as presented, with the exception of pagination.Authorized licensed use limited to: National Chung Hsing Univ.. Downloaded on March 27,2023 at 07:07:45 UTC from IEEE Xplore.Restrictions apply.\n",
      "====================\n",
      "翻譯結果:\n",
      " 就數據集大小而言，VesselReID接近於人物重新識別（person re-ID）最受歡迎的數據集之一，即Market1501[24]。這兩個數據集均具有相同數量的攝像頭。本文已經被接受包含在未來期刊中。內容最終呈現，唯一例外的是分頁。授權許可使用僅限於：國立中興大學。從IEEE Xplore於2023年3月27日07:07:45 UTC下載。限制適用。\n",
      "====================\n",
      "====================\n",
      "原文:\n",
      " ZHANG et al.: UNSUPERVISED MARITIME VESSEL RE-IDENTIFICATION WITH MCL 7\n",
      "TABLE I\n",
      "COMPARISON BETWEEN VesselReID AND OTHER DATASETS\n",
      "locations and over 30k images.The intra-location variation of\n",
      "VesselReID is supposed to be larger than that of Market1501,\n",
      "since the images annotated with the same location may be\n",
      "taken from various viewpoints.When compared with the Boat Re-ID dataset [1], the most\n",
      "notable advantage of VesselReID is that it has sufficient vessel\n",
      "instances for deep learning-based research.In addition, there\n",
      "are two distinguishable features for the VesselReID dataset.Firstly, all images in VesselReID are captured for real, while\n",
      "most of the images in Boat ReID are augmented images which\n",
      "are highly redundant with their corresponding source images.Secondly, VesselReID is composed of original vessel images\n",
      "captured in different external conditions such as viewpoint,\n",
      "weather, and lighting conditions, which make it much more\n",
      "challenging than Boat ReID that only consists of images with\n",
      "the cluttered background cropped out.\n",
      "====================\n",
      "翻譯結果:\n",
      " ZHANG等人：使用MCL 7的無監督海上船舶再識別\n",
      "表格I\n",
      "VesselReID與其他數據集的比較\n",
      "VesselReID數據集包含來自不同地點的超過30,000張圖像。由於標註為同一地點的圖像可能來自不同的視角，因此VesselReID的內部位置變化應該比Market1501更大。與Boat Re-ID數據集相比，VesselReID最明顯的優勢在於具有足夠的船舶實例，可以進行基於深度學習的研究。此外，VesselReID數據集具有兩個可區分的特點。首先，VesselReID中的所有圖像都是真實拍攝的，而Boat ReID中的大部分圖像都是高度冗余的增強圖像與其對應的源圖像。其次，VesselReID由不同外部環境拍攝的原始船舶圖像組成，例如視角、天氣和照明條件，這使其比只包含裁剪過雜亂背景圖像的Boat ReID更具挑戰性。\n",
      "====================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "原文:\n",
      " Besides, benefiting from\n",
      "the way the dataset is constructed, it is possible to expand the\n",
      "dataset with the newly uploaded images on the website at low\n",
      "cost.V. E XPERIMENTS\n",
      "In this section, we report experimental results on our\n",
      "self-built VesselReID dataset to evaluate the effectiveness of\n",
      "the proposed method.A.Implementation Details\n",
      "1) Encoder: We use a modified ResNet-50 as the default\n",
      "encoder fθas in [17] and [16], where the classifier (a fully-\n",
      "connected layer) is removed and a batch normalisation (BN)\n",
      "layer [64] is added at the end of the network.We initialise the\n",
      "encoder fθfor the first training epoch, i.e.f0\n",
      "θ, with parameters\n",
      "pre-trained on the ImageNet dataset before the start of each\n",
      "training session.2) Pseudo-Label Generator: Based on the features\n",
      "extracted with the encoder ft−1\n",
      "θtrained in the previous epoch,\n",
      "we generate the pseudo label ykfor each unlabeled vessel\n",
      "image xkin the dataset by a label generator G, i.e., yk≜\n",
      "G(ft−1\n",
      "θ(xk)).Therefore, the views ˜x2k−1and˜x2k, augmented\n",
      "in different manners from xk, share the same pseudo label\n",
      "yk.\n",
      "====================\n",
      "翻譯結果:\n",
      " 除此之外，由於數據集的構建方式，我們可以以較低的成本將新上傳的圖片擴展到數據集中。在這節中，我們使用自行建構的VesselReID數據集來評估所提出方法的有效性。實現細節如下：1) 編碼器：我們使用修改過的ResNet-50作為默認的編碼器fθ，該方法與[17]和[16]相同，並且移除了分類器（即全連接層），在網絡結尾添加了批標準化層[64]。在每個訓練階段開始前，初始化編碼器fθ的參數為預先在ImageNet數據集上訓練過的參數f0θ。2)偽標籤生成器：基於在前一個訓練階段中使用編碼器ft-1θ提取的特徵，我們使用一個標籤生成器G，將每個未標記的船舶圖像xk產生偽標籤yk，即yk≜G(ft-1θ(xk))。因此，從xk以不同方式擴增的視圖˜x2k-1和˜x2k共享相同的偽標籤yk。\n",
      "====================\n",
      "====================\n",
      "原文:\n",
      " We adopt DBSCAN [65] with Jaccard distance [66] as\n",
      "the label generator G. Generally, the clustering process is\n",
      "controlled by the maximum distance ϵbetween neighbours,\n",
      "which is set to 0.6 as a baseline.The unclustered instances are\n",
      "ignored during the training.The minimum number of samples\n",
      "in a neighbourhood for a point to be considered as a core point\n",
      "is set to 4.For calculating the Jaccard distance, we set k1 to\n",
      "30 and k2 to 6 in line with [66].3) Sampling Strategy: PK sampler, which randomly selects\n",
      "Pidentities and Kinstances of each identity has been widely\n",
      "used in both supervised and unsupervised person/vehicle\n",
      "re-ID.In this paper, we adopt PK sampler with K=4 to\n",
      "build a mini-batch of vessel samples.Pis then determined\n",
      "according to the batch size as P=BatchSize /K.4) Data Augmentations: The data augmentations (noted as\n",
      "Ain Fig.2) adopted in this paper are described in detail using\n",
      "PyTorch notations.In line with [17], we use the geometric\n",
      "augmentations and RandomErasing [67] as a baseline setting.\n",
      "====================\n",
      "翻譯結果:\n",
      " 我們採用帶有Jaccard距離的DBSCAN [65] 作為標籤生成器G。通常，聚類過程受到最大距離ϵ的控制，基於基線設置，該值設置為0.6。在訓練過程中，未聚類的實例將被忽略。將被視為核心點的鄰域中的最小樣本數量設置為4。為了計算Jaccard距離，我們按照[66]中的設置，將k1設置為30，將k2設置為6。\n",
      "\n",
      "3)採樣策略：PK取樣器在監督和無監督人/車輛重新識別中得到了廣泛使用，它隨機選擇每個身份的P身份和K實例。在本文中，我們採用K = 4的PK採樣器來構建一個較小的船隻樣本的mini-batch。然後根據批量大小確定P=BatchSize / K。\n",
      "\n",
      "4)數據增強：本文中採用的數據增強方法（在圖2中標註為A）使用PyTorch符號進行詳細描述。基於[17]，我們使用幾何增強和RandomErasing [67]作為基線設置。\n",
      "====================\n",
      "====================\n",
      "原文:\n",
      " Geometric augmentations applied in this paper include Resize,\n",
      "RandomHorizontalFlip as well as RandomCrop.Each image is\n",
      "first resized to a uniform size of 180 ×256.Then, it is randomly\n",
      "flipped along the horizontal direction with a probability of 0.5.The last geometric augmentation operation is RandomCrop\n",
      "with a padding of 10 and a target size of 180 ×256.Finally,\n",
      "RandomErasing is applied with a scale parameter in the\n",
      "interval of [0.02, 0.3] and a probability of 0.5.5) Optimiser: Adam optimiser is adopted to train our vessel\n",
      "re-ID model based on MCL with a weight decay of 0.0005.The learning rate is initialised as 0.00035 and reduced to\n",
      "1/10 of its previous value every 20 epochs.We train the\n",
      "models for 50 epochs and 200 iterations per epoch in the\n",
      "experiments.6) Evaluation Settings: For evaluation, the images are first\n",
      "resized to 180 ×256 and then normalised with the RGB mean\n",
      "and the standard deviation.The vessel re-ID performance\n",
      "is evaluated by mean Average Precision (mAP) as well as\n",
      "Cumulative Matching Characteristic (CMC) Rank-1, Rank-5\n",
      "and Rank-10 scores based on ℓ2normalised features generated\n",
      "by the learned encoder fθ.\n",
      "====================\n",
      "翻譯結果:\n",
      " 本文中使用的幾何增強技術包括 Resize、RandomHorizontalFlip 以及 RandomCrop。首先，每個圖像都被調整為統一大小的 180 × 256。然後，以概率 0.5 隨機在水平方向上翻轉圖像。最後一個幾何增強操作是使用 10 的填充和 180 × 256 的目標大小進行 RandomCrop。最後，使用比例在 [0.02, 0.3] 之間且概率為 0.5 的 RandomErasing。5）優化器：採用帶有 MCL 的 Adam 优化器来訓練我们的血管重识别模型，其权重衰减为 0.0005。學習率最初設置為 0.00035，每 20 轮将其减少到上一次的 1/10。在實驗中每個 epoch 訓練 50 個 epoch，每個 epoch 200 次迭代。6）評估设置：為了進行評估，首先將圖像大小調整為 180 × 256，然後使用 RGB 平均值和標準差對其進行正規化。通過 ℓ2 正規化特徵生成的平均平均精度（mAP）以及累積匹配特徵（CMC）Rank-1、Rank-5 和 Rank-10 分數來評估血管重识別性能。\n",
      "====================\n",
      "====================\n",
      "原文:\n",
      " Note that there is no validation\n",
      "set in the normal split of our VesselReID dataset, thus we\n",
      "evaluate the performance on the test set during the training\n",
      "and choose the best checkpoint as the final trained model\n",
      "following the previous re-ID works [15], [16], [17], [18], [19],\n",
      "[20], [21], [22].No post-processing (e.g.re-ranking [66]) is\n",
      "used in ablation studies.B.Comparison\n",
      "In this section, we compare the proposed MCL method with\n",
      "state-of-the-art contrastive methods [15], [16], [17], [19], [22],\n",
      "[43], [44], [45], [46], [47], [48] that have been successfully\n",
      "applied in person or vehicle re-ID.Experimental results with\n",
      "the normal and the extra splits of VesselReID are separately\n",
      "listed in Tab.IIfor fair comparison.To evaluate the above methods on VesselReID, we only\n",
      "make necessary adjustments on parameter settings, such as\n",
      "target width and height for Resize, while keeping most of the\n",
      "settings consistent with the original papers.We only reproduce the camera-agnostic version of ICE [19]\n",
      "and PPLR [44], and the unsupervised version of SpCL [17],\n",
      "SECRET [46] and HCD [43], as there exists no meaning-\n",
      "ful camera information in our VesselReID dataset and no\n",
      "annotated dataset available for domain adaptation.\n",
      "====================\n",
      "翻譯結果:\n",
      " 注意到我們的VesselReID數據集中沒有驗證集，因此我們在訓練期間對測試集進行性能評估，並根據以前的re-ID作品[15]，[16]，[17]，[18]，[19] ，[20]，[21]，[22]選擇最佳檢查點作為最終訓練模型。在消融研究中不使用後處理（例如重新排序[66]）。B.比較\n",
      "\n",
      "在本節中，我們將提出的MCL方法與已成功應用於人員或車輛重新識別的最先進對比方法[15]，[16]，[17]，[19]，[22]，[43]，[44]，[45 ]，[46]，[47]，[48]進行比較。正常和額外拆分的VesselReID的實驗結果分別列在表II中，以進行公平比較。為了在VesselReID上評估上述方法，我們僅對參數設置進行必要的調整，例如Resize的目標寬度和高度，同時保持大部分設置與原始論文一致。我們僅重現ICE [19]和PPLR [44]的相機不可知版本，以及SpCL [17]，SECRET [46]和HCD [43]的無監督版本，因為在我們的VesselReID數據集中不存在有意義的相機信息，也沒有可用於域適應的注釋數據集。\n",
      "====================\n",
      "====================\n",
      "原文:\n",
      " Besides,\n",
      "we experimentally find that not all methods produce better\n",
      "performance with larger batch size, and thus it is not a good\n",
      "choice to set a uniform batch size for them.This article has been accepted for inclusion in a future issue of this journal.Content is final as presented, with the exception of pagination.Authorized licensed use limited to: National Chung Hsing Univ.. Downloaded on March 27,2023 at 07:07:45 UTC from IEEE Xplore.Restrictions apply.\n",
      "====================\n",
      "翻譯結果:\n",
      " 此外，我們的實驗發現並非所有方法在使用較大的批次大小時都能產生更好的效能，因此為它們設定一個統一的批次大小並不是一個好的選擇。本文章已經被接受，將會被收錄在未來的某個期刊中。文章內容最終版本如所呈現的，惟頁碼除外。授權許可使用範圍有限，僅限國立中興大學下載，於2023年3月27日07:07:45 UTC從IEEE Xplore下載。限制條款適用。\n",
      "====================\n",
      "====================\n",
      "原文:\n",
      " 8 IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS\n",
      "TABLE II\n",
      "COMPARISONS WITH THE STATE -OF-THE-ARTCONTRASTIVE METHODS ON VesselReID.BS, MB, RR R EPRESENT\n",
      "BATCH SIZE, MEMORY BANK AND RE-RANKING RESPECTIVELY\n",
      "Re-ranking [66] is a popular trick to further boost the re-ID\n",
      "performance.It utilises information of nearest neighbours and\n",
      "can be easily combined with other ranking methods.Although\n",
      "no re-ranking is conducted in our ablation experiments listed\n",
      "in Section V-C, we report experimental results both with and\n",
      "without re-ranking for full comparison in Tab.II.The top half\n",
      "of Tab.II reports the evaluation results without re-ranking.As shown in Tab.II, MMCL [16] and SpCL [17] which\n",
      "perform well in person re-ID cannot produce competitive\n",
      "results in vessel re-ID.This is mainly because data aug-\n",
      "mentation and sampling strategy can only mitigate the neg-\n",
      "ative effects of the HPP to a certain extent, which is not\n",
      "enough for the challenging vessel re-ID task.IICS [47] and\n",
      "MetaCam+DSCE [48] perform even worse due mainly to the\n",
      "over-reliance of these methods on camera information, which\n",
      "is missing in VesselReID.\n",
      "====================\n",
      "翻譯結果:\n",
      " 表二是對船舶再識別的最新技術方法進行了比較，其中批次大小（BS）、內存庫（MB）和重新排序（RR）分別表示批次大小、內存庫和重新排序。重新排序是進一步提高再識別性能的一種流行技巧。它利用最近鄰居的信息，並且可以與其他排名方法輕松結合使用。雖然在我們在第五節中列舉的消融實驗中沒有進行重新排序，但我們報告了有與沒有重新排序的實驗結果，以進行全面比較。\n",
      "\n",
      "表中上半部分報告了在沒有重新排序的情況下的評估結果。如表所示，在人員再識別方面表現良好的 MMCL [16] 和 SpCL [17] 在船舶再識別方面無法產生競爭力的結果。這主要是因為數據增強和抽樣策略只能在一定程度上緩解 HPP 的負面影響，這對於具有挑戰性的船舶再識別任務來說是不夠的。IICS [47] 和 MetaCam+DSCE [48] 的表現甚至更差，主要是因為這些方法過度依賴攝像機信息，而該信息在船舶再識別中並不適用。\n",
      "====================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "原文:\n",
      " ICE [19] achieves the closest results\n",
      "with our MCL among previous methods, indicating that both\n",
      "cluster-level and instance-level discrimination are indispens-\n",
      "able for vessel re-ID.Such results are consistent with our\n",
      "findings in Section V-C.In addition, we reproduce SimCLR\n",
      "[54] for vessel re-ID and find that conventional instance-level\n",
      "representation learning cannot achieve good results due inpart to the existence of the HPP.By taking more samples as\n",
      "positives in instance-level discrimination, our MCL effectively\n",
      "solves the HPP and outperforms all previous methods without\n",
      "a memory bank.In fact, what the memory bank actually\n",
      "provides are enough negative samples for contrastive learning,\n",
      "and thus the performance of the MCL may be degraded in the\n",
      "absence of a memory bank.This problem can be solved by\n",
      "using a larger batch size capable of involving more negative\n",
      "samples in each iteration (see Section V-C.7 for details).It is worth noting that most of the methods focusing on\n",
      "pseudo-label refinement [15], [44], [45], [46] show relatively\n",
      "better performance in vessel re-ID.\n",
      "====================\n",
      "翻譯結果:\n",
      " ICE[19]在我們的MCL之中表現最佳，比先前的方法更加接近，這表明聚類和實例層面的區分對於船舶 ID 很重要。這些結果與我們在第V-C節中的發現一致。此外，我們再次使用SimCLR[54]來進行船舶ID，發現傳統的實例級表示學習由於存在HPP而無法取得良好的結果。通過在實例級區分中採用更多樣本作為正樣本，我們的MCL有效地解決了HPP的問題，並且在沒有內存庫存的情況下優於所有先前的方法。實際上，內存庫存提供的是足夠的負樣本用於對比學習，因此沒有內存庫存可能會降低MCL的性能。可以通過使用大型的批次大小來解決此問題，從而在每次迭代中涉及更多的負樣本（有關詳細信息請參見第V-C.7節）。值得注意的是，大多數關注偽標籤精煉的方法[15]，[44]，[45]，[46]在船舶ID上表現相對較好。\n",
      "====================\n",
      "====================\n",
      "原文:\n",
      " We think it is not surprising\n",
      "as improving the quality of pseudo-labels can also solve the\n",
      "HPP problem to some extent.In addition, to evaluate the generalisation ability of the\n",
      "MCL, we test it on the small-scale Boat ReID dataset [1].The evaluation results listed in Tab.IIIshow that the vessel\n",
      "re-ID model trained on the VesselReID dataset using our MCL\n",
      "algorithm generalises well to the Boat ReID dataset, achieving\n",
      "an average mAP score of 63.6% on 7 subsets containing\n",
      "different types of vessel images, i.e.the original vessel images\n",
      "and the images augmented in various manners.Note thatThis article has been accepted for inclusion in a future issue of this journal.Content is final as presented, with the exception of pagination.Authorized licensed use limited to: National Chung Hsing Univ.. Downloaded on March 27,2023 at 07:07:45 UTC from IEEE Xplore.Restrictions apply.\n",
      "====================\n",
      "翻譯結果:\n",
      " 我們認為這並不奇怪，因為改進伪标签的質量也可以在一定程度上解決HPP問題。此外，為了評估MCL的泛化能力，我們在小型Boat ReID數據集[1]上進行測試。表III中列出的評估結果顯示，使用我們的MCL算法在VesselReID數據集上訓練的船舶重新識別模型在Boat ReID數據集上具有良好的泛化能力，平均mAP得分為63.6％，包含不同類型的船舶圖像，即原始船舶圖像和以不同方式增強的圖像。請注意，本文已獲接受並包含在未來期刊的內容中，除了分頁之外，內容已完成，授權使用僅限於：國立中興大學。已從IEEE Xplore於2023年3月27日07:07:45 UTC下載。限制適用。\n",
      "====================\n",
      "====================\n",
      "原文:\n",
      " ZHANG et al.: UNSUPERVISED MARITIME VESSEL RE-IDENTIFICATION WITH MCL 9\n",
      "Fig.4.Ranking results on VesselReID test set.The green and the red boxes indicate the correct and the incorrect retrieval results, respectively.TABLE III\n",
      "EVALUATION RESULTS ON THE SMALL -SCALE BOAT ReID D ATASET [1]\n",
      "there is a significant performance degradation when all vessel\n",
      "images in Boat ReID are used for testing.This is mainly\n",
      "because it is a challenge to re-identify vessels in various styles\n",
      "of images using the re-ID model trained with the images\n",
      "augmented in the same way.To further verify the effectiveness of MCL, we visualise\n",
      "some retrieval results for 4 randomly selected query vessel\n",
      "images in Fig.4.Although a vessel appears with different\n",
      "backgrounds in VesselReID, MCL accurately retrieves images\n",
      "from the gallery that share the same vessel identity with\n",
      "the query image.However, there still exist some incorrect\n",
      "retrievals.The main reason is that vessels with different IMO\n",
      "numbers may be produced with the same model, making it\n",
      "almost impossible to distinguish them by visual appearance.\n",
      "====================\n",
      "翻譯結果:\n",
      " 張等人: MCL 9下的非監督式海上船舶再識別\n",
      "圖4.在VesselReID測試集上的排名結果。綠色和紅色框分別表示正確和錯誤的檢索結果。表III\n",
      "在Boat ReID數據集上進行全部船舶圖像測試時，性能明顯下降。這主要是因為使用以同樣方式增強圖像訓練的再識別模型難以識別各種風格圖像中的船舶。為進一步驗證MCL的有效性，我們在圖4中展示了4個隨機選擇的查詢船舶圖像的檢索結果。儘管VesselReID中船舶在不同背景下出現，但MCL能夠準確地檢索相同的船舶身份與查詢圖像相同的圖像庫。然而，仍有一些不正確的檢索結果。主要原因是具有不同IMO號碼的船舶可能使用相同的模型生產，使得它們在外觀上幾乎無法區分。\n",
      "====================\n",
      "====================\n",
      "原文:\n",
      " Our MCL can also handle the HPP for vehicle re-ID.Tab.IV\n",
      "shows the comparative results on the popular VeRi776 dataset\n",
      "[25].It can be seen that MCL outperforms several state-of-\n",
      "the-art methods in the field of unsupervised vehicle re-ID.TABLE IV\n",
      "COMPARISONS WITH THE STATE -OF-THE-ARTCONTRASTIVE\n",
      "METHODS ON VeR I776 [ 25]\n",
      "C. Ablation Studies\n",
      "In this section, we study the effectiveness of the main com-\n",
      "ponents and the impact of different hyper-parameter settings\n",
      "used by the proposed method.The normal split of VesselReID\n",
      "is utilised for experiments.1) Baseline: We select as our baseline the unsupervised\n",
      "version of Supervised Contrastive Learning (SCL) [63], which\n",
      "replaces the ground-truth labels with the pseudo labels gener-\n",
      "ated by clustering to enable an unsupervised learning.For the\n",
      "settings of label generator and data augmentation, we refer\n",
      "to SpCL [17] that performs well in unsupervised person\n",
      "re-ID.Such settings have been described in detail in previous\n",
      "sections.We use the PK sampler to randomly select Pvessel\n",
      "identities and Kimages of each identity for a mini-batch of\n",
      "128 samples.\n",
      "====================\n",
      "翻譯結果:\n",
      " 我們的 MCL 也可以處理車牌號碼 plate (HPP) 的車輛重新識別 (re-ID)。表 IV 顯示了在流行的 VeRi776 資料集 [25] 上的比較結果。可以看出，MCL 在無監督車輛 re-ID 領域中的表現優於幾種最先進的方法。表 IV 比較了 VeR I776 [25] 上對比方法的表現。\n",
      "\n",
      "C. 消融研究\n",
      "在這一部分中，我們研究了所提出的方法的主要組件的有效性以及不同超參數設置對其影響。我們使用 VesselReID 的正常分裂進行實驗。1）基線：我們選擇監督對比學習（SCL）[63]的無監督版本作為基線，將真實標籤替換為通過聚類生成的伪標籤以實現無監督學習。對於標籤生成器和數據增強的設置，我們參考了在無監督人物重新識別中表現良好的 SpCL [17]。這些設置在前面的章節中已詳細描述。我們使用 PK 抽樣器隨機選擇 Pvessel 身份和每個身份的 K 張圖像用於一個小批次的 128 個樣本。\n",
      "====================\n",
      "====================\n",
      "原文:\n",
      " As shown in Tab.V, our baseline achieves 7.6%\n",
      "mAP and 17.2% Rank-1 score on VesselReID.2) Effectiveness of Instance-Level Discrimination: Different\n",
      "with SCL, the ICL module in this paper not only computes\n",
      "contrastive losses for all positive pairs that share the same\n",
      "pseudo label, but also removes other positives from negative\n",
      "items when optimising the model with the current positive pair,This article has been accepted for inclusion in a future issue of this journal.Content is final as presented, with the exception of pagination.Authorized licensed use limited to: National Chung Hsing Univ.. Downloaded on March 27,2023 at 07:07:45 UTC from IEEE Xplore.Restrictions apply.\n",
      "====================\n",
      "翻譯結果:\n",
      " 如圖V所示，在VesselReID上，我們的基線達到了7.6%的mAP和17.2%的Rank-1分數。2）實例級別區分的有效性：與SCL不同，本文中的ICL模塊不僅計算具有相同偽標籤的所有正對數據的對比損失，還在優化當前正對數據時，從負項目中刪除其他正對數據。本文已被接受，包含在未來的期刊中。內容是最終展示的，但頁碼除外。授權許可使用僅限於：國立中興大學。從IEEE Xplore於2023年3月27日07:07:45 UTC下載。限制適用。\n",
      "====================\n",
      "====================\n",
      "原文:\n",
      " 10 IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS\n",
      "TABLE V\n",
      "IMPACT OF MAINCOMPONENTS OF OURFRAMEWORK\n",
      "Fig.5.T-SNE [68] visualization of samples belonging to 10 random selected\n",
      "vessel identities in VesselReID test set.Features learned by our MCL (left)\n",
      "are with more significant inter-cluster variation than SCL (right), showing that\n",
      "MCL works better in vessel re-ID.as shown in Equ.2.As such, the proposed method improves\n",
      "the efficiency of the optimisation while solving the HPP caused\n",
      "by merely conducting the instance-level contrastive learning.It is clear in Tab.Vthat by setting a label mask in the\n",
      "denominator of Equ.2, the performance of the vessel re-ID\n",
      "model improves by 27.6% mAP and 35.5% Rank-1 score\n",
      "compared to the baseline method.3) Effectiveness of Intra-Batch Cluster-Level Discrimina-\n",
      "tion: While most of the previous methods do cluster-level\n",
      "contrastive learning with the aid of a global memory bank, our\n",
      "CCL module conducts intra-batch cluster-level discrimination\n",
      "with no need of memory bank.\n",
      "====================\n",
      "翻譯結果:\n",
      " 第十卷IEEE智能交通系統研究中的表格5我們框架主要部分的影響。圖5顯示了VesselReID測試集中屬於10個隨機選定船舶身份的樣本的T-SNE[68]可視化。我們MCL所學到的特徵(左)具有更顯著的簇間差異性，優於SCL(右)，顯示MCL在船舶再識別方面效果更好，正如Equ.2所示。因此，本提出的方法改進了優化的效率，同時解決了僅通過進行實例級對比學習而引起的HPP問題。從表格V中可以清楚地看出，在Equ.2的分母中設置標籤掩模，與基準方法相比，船舶再識別模型的性能提高了27.6%的mAP和35.5%的排名1分數。3）對內部批次簇級別區分效果的有效性：雖然大多數先前的方法借助全局存儲器進行簇級對比學習，但我們的CCL模塊在不需要存儲器的情況下進行內部批次簇級區分。\n",
      "====================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "原文:\n",
      " During the training, Equ.5\n",
      "pulls an anchor sample closer to its positive centroid and\n",
      "pushes it away from all negative centroids in the current\n",
      "mini-batch at the same time, which avoids increasing the\n",
      "distances between positive pairs.The HPP is then solved\n",
      "since all samples belonging to the same intra-batch cluster\n",
      "are taken into account to calculate the cluster centroid.Exper-\n",
      "imental results show that the proposed intra-batch cluster-level\n",
      "contrastive learning method achieves 29 .8% mAP and 50.7%\n",
      "Rank-1 score on VesselReID, which outperforms the baseline\n",
      "significantly.4) Effectiveness of Multi-Level Discrimination: The bottom\n",
      "row of Tab.Vshows that our MCL framework performs better\n",
      "than single-level contrastive learning in vessel re-ID.This\n",
      "is not surprising because the roles of the instance-level and\n",
      "the cluster-level discrimination modules are complementary\n",
      "to some extent.The former pays more attention to detailed\n",
      "differences between samples and thus is likely to be affected\n",
      "by noisy samples.\n",
      "====================\n",
      "翻譯結果:\n",
      " 在訓練過程中，Equ.5 同時將錨定樣本拉近到其正確的重心，並將其推離當前 mini-batch 中所有負重心，從而避免增加正向配對之间的距離。由此可以解決 HPP（Hard Positive Pair）問題，因為考慮到同一 intra-batch 群集中的所有樣本以計算其群集重心。實驗結果表明，所提出的 intra-batch 群集級對比學習方法在 VesselReID 上實現了 29.8％ 的 mAP 和 50.7％ 的 Rank-1 得分，明顯優於基線。\n",
      "\n",
      "4）多級別的區分效果：Tab.V 的底部行顯示，我們的 MCL 框架在船舶重新識別中比單級對比學習的表現更好。這不足為奇，因為實例級和群集級判別模塊的角色在某種程度上是互補的。前者更關注樣本之間的詳細差異，因此很可能會受到雜訊樣本的影響。\n",
      "====================\n",
      "====================\n",
      "原文:\n",
      " In particular, vessel identities present high\n",
      "similarities in appearance and different views of the same ves-\n",
      "sel vary a lot from each other, which makes the pseudo labels\n",
      "generated by clustering not accurate enough.Benefiting from\n",
      "the fact that a cluster centroid is a combination of augmented\n",
      "views of all original vessel images in an intra-batch cluster,\n",
      "cluster-level discrimination is more robust to noisy samples.As a result, the proposed MCL outperforms the competing\n",
      "methods for unsupervised vessel re-ID.In addition, by com-\n",
      "paring the individual performance of ICL and CCL, ICL is\n",
      "more effective for vessel re-ID.Presumably, this is related\n",
      "to the nature of vessel re-ID, where detailed information is\n",
      "needed to distinguish different vessel identities.Fig.5shows\n",
      "the visualisation of the samples belonging to 10 randomly\n",
      "selected vessel identities in the test set of VesselReID, which\n",
      "demonstrates that the features learned by MCL lead to a better\n",
      "identification of different vessels than the baseline method.\n",
      "====================\n",
      "翻譯結果:\n",
      " 尤其是在外觀方面，船舶身份具有高度的相似性，同一艘船的不同視圖之間差異很大，這使得由聚類生成的虛擬標籤並不夠準確。由於聚類重心是一個將批內聚類中所有原始船舶圖像的增強視圖組合在一起的組合，因此聚類級別的區別度對於有噪聲的樣本更為強大。因此，所提出的MCL在無監督船舶重新識別方面優於競爭方法。此外，通過比較ICL和CCL的個別表現，ICL對於船舶重新識別更有效。大概是因為船舶重新識別需要詳細的信息來區分不同的船舶身份。圖5展示了VesselReID測試集中10個隨機選取的船舶身份的樣本可視化結果，顯示MCL所學習的特徵比基線方法更好地區分了不同的船舶。\n",
      "====================\n",
      "====================\n",
      "原文:\n",
      " 5) Impact of ϵin Clustering: The value of ϵcontrols the\n",
      "maximum distance between neighbours in the clustering pro-\n",
      "cess.A larger ϵmeans that more samples will be clustered into\n",
      "one group and thus each sample will have more neighbours\n",
      "as their positives.In person re-ID, ϵis commonly set to 0.6.Instances of different person identities can be distinguished\n",
      "from each other according to their external characteristics,\n",
      "especially the colour of their clothes.However, the difference\n",
      "between maritime vessels is smaller, such that large ϵleads to\n",
      "poor clustering performance.As shown in Tab.VI, we first decrease the value of ϵto\n",
      "0.4 for ICL.Unsurprisingly, we can see that vessel re-ID\n",
      "models perform better (38.7% vs35.2% in terms of mAP) with\n",
      "a softer setting for pseudo-label generation, which is consistent\n",
      "with our analysis.When we reduce ϵto 0.3, the accuracy of\n",
      "vessel re-ID is further improved (41.4% vs38.7% in terms of\n",
      "mAP).An excessively small ϵmay be harmful to the learning\n",
      "process as shown in the last row of Tab.\n",
      "====================\n",
      "翻譯結果:\n",
      " VI.\n",
      "5) ϵ在聚類中的影響： ϵ的值控制著聚類過程中鄰居之間的最大距離。更大的ϵ意味著更多樣本會被聚類到一組中，因此每個樣本會有更多的鄰居作為其正例。在人物再辨識中，ϵ通常設置為0.6。不同人的身份可以根據其外部特徵，尤其是衣服顏色來區分。然而，海上船只之間的差異較小，因此大的ϵ會導致聚類性能較差。如圖VI所示，我們首先將ICL的ϵ值降低到0.4。不出所料，我們可以看到在一個更軟的偽標籤生成設置下，船只再辨識模型表現得更好（以mAP為35.2% vs. 38.7%）。當我們將ϵ進一步減少為0.3時，船只再辨識的準確度進一步提高（以mAP為38.7% vs. 41.4%）。然而，過小的ϵ可能會對學習過程有害，如圖VI的最後一行所示。\n",
      "====================\n",
      "====================\n",
      "原文:\n",
      " VI.This is because\n",
      "in this case, the criterion of clustering is too strict to regard\n",
      "a sample as a cluster centroid and thus most of the training\n",
      "samples are discarded during the training.CCL performs slightly worse when ϵbecomes smaller.This\n",
      "is possibly because a cluster centroid is not so robust when\n",
      "the number of samples in the corresponding cluster decreases.The reason that we do not report the result of cluster-level\n",
      "discrimination with ϵ=0.2 is that too many samples are\n",
      "considered as outliers.As a result, the number of clusters is\n",
      "less than 32 and thus there are not enough vessel identities for\n",
      "the PK sampler.For the proposed MCL method, we empirically find that ϵ=\n",
      "0.4 is a good choice for making a balance between ICL and\n",
      "CCL as shown in Tab.VII.This is also reflected in the change\n",
      "of the number of clusters (the right column in Tab.VII).When\n",
      "ϵis set to 0.4, images in the training set are clustered into\n",
      "602 clusters, which is much closer to the number of ground\n",
      "truth vessel identities.\n",
      "====================\n",
      "翻譯結果:\n",
      " 六. 原因\n",
      "在這種情況下，聚類標準過於嚴格，無法將樣本視為聚類中心，因此在訓練過程中大部分的訓練樣本都被丟棄。當ε變小時，CCL的表現稍微差一些，這可能是因為當對應的聚類中樣本數量減少時，聚類中心不太穩健。我們沒有報告ε=0.2的聚類水平區分結果，是因為太多的樣本被視為異常值，因此聚類數目少於32，沒有足夠的血管身份識別信息供PK採樣器使用。對於所提出的MCL方法，我們在實驗中發現，將ε設為0.4可以很好地平衡ICL和CCL，如表VII所示。這也反映在聚類數量的變化上（表VII中的右列）。當ε設為0.4時，訓練集中的圖像被聚類成602個聚類，這與地面實況血管的數量相差不大。\n",
      "====================\n",
      "====================\n",
      "原文:\n",
      " 6) Impact of Data Augmentations: As is mentioned above,\n",
      "we follow the augmentation settings adopted in SpCL [17]\n",
      "to establish a baseline noted as Abase.MMCL [16] adopts a\n",
      "more complicated data augmentation module, which consists\n",
      "of three additional operations, i.e.CamStyle [69], Random-\n",
      "Rotation and ColorJitter.Since the images in the Vessel-\n",
      "ReID dataset were downloaded from the Internet and only\n",
      "annotated with coarse locations, CamStyle is not suitable as\n",
      "a member of data augmentations for vessel re-ID in this\n",
      "paper.The main focus of this section is how RandomRota-\n",
      "tion andColorJitter affect the performance of vessel re-ID.We implement RandomRotation with a degree of [−10, 10]This article has been accepted for inclusion in a future issue of this journal.Content is final as presented, with the exception of pagination.Authorized licensed use limited to: National Chung Hsing Univ.. Downloaded on March 27,2023 at 07:07:45 UTC from IEEE Xplore.Restrictions apply.\n",
      "====================\n",
      "翻譯結果:\n",
      " 6) 資料增強的影響：如上所述，我們遵循 SpCL [17] 採用的增強設置作為基準線，標記為Abase.MMCL [16]採用了更複雜的資料增強模組，其中包括三個額外的操作，即CamStyle [69]、RandomRotation和ColorJitter。因為Vessel-ReID數據集中的圖像是從網上下載的，並且只有粗略的位置標註，所以在本文中，CamStyle不適用於Vessel Re-ID的資料增強成員。本節的主要重點是隨機旋轉和ColorJitter如何影響船舶重新識別的性能。我們使用[-10, 10]的角度實現了RandomRotation。本文已被接受，包含在此期刊的未來問題中。內容正如呈現的那樣，除了分頁之外。經授權有限使用，僅限國立中興大學。從IEEE Xplore於2023年3月27日07:07:45 UTC下載。限制適用。\n",
      "====================\n",
      "====================\n",
      "原文:\n",
      " ZHANG et al.: UNSUPERVISED MARITIME VESSEL RE-IDENTIFICATION WITH MCL 11\n",
      "TABLE VI\n",
      "EFFECT OF ϵINSINGLE -LEVEL CONTRASTIVE LEARNING\n",
      "TABLE VII\n",
      "EFFECT OF ϵINMULTI -LEVEL CONTRASTIVE LEARNING\n",
      "TABLE VIII\n",
      "EFFECT OF DATA AUGMENTATIONS .RR, CJ, RHF AND RE D ENOTE\n",
      "RANDOM ROTATION , COLOR JITTER , RANDOM HORIZONTAL FLIP AND\n",
      "RANDOM ERASING RESPECTIVELY\n",
      "TABLE IX\n",
      "EFFECT OF BATCH SIZE\n",
      "andColorJitter with the {brightness ,contrast ,saturation }\n",
      "strength of {0.2,0.2,0.2}respectively.Based on the statistics\n",
      "in Tab.VIII, we can conclude that ColorJitter is helpful for\n",
      "training a powerful vessel re-ID model while RandomRotation\n",
      "works in reverse.By adding ColorJitter toAbase, i.e.AMCL\n",
      "in Tab.VIII, our MCL further improves the accuracy of vessel\n",
      "re-ID by 1.5% mAP and 1.1% Rank-1 score.We further\n",
      "evaluate the performance by removing RandomHorizontalFlip\n",
      "and RandomErasing fromAMCL respectively, and find that\n",
      "they are both necessary for unsupervised vessel re-ID.7) Impact of Batch Size: Tab.IXshows the impact of\n",
      "batch size on the proposed MCL method.\n",
      "====================\n",
      "翻譯結果:\n",
      " 張等人：MCL 11無監督式船舶再識別\n",
      "\n",
      "表格VI\n",
      "ϵinsingle 的單層對比學習影響\n",
      "\n",
      "表格VII\n",
      "ϵinmulti 的多層對比學習影響\n",
      "\n",
      "表格VIII\n",
      "數據增強（RR、CJ、RHF和RE）的效果。\n",
      "其中，“RR”表示隨機旋轉，“CJ”表示隨機顏色更改，“RHF”表示隨機水平翻轉，“RE”表示隨機擦除。\n",
      "\n",
      "表格IX\n",
      "批量大小的影響。\n",
      "\n",
      "其中，“CJ”是指亮度，對比度和飽和度的強度分別為0.2、0.2、0.2。根據表VIII的統計數據，我們可以得出結論：對於訓練強大的船舶再識別模型，CJ有幫助，而RR相反。通過在基本模型AMCL的基礎上添加CJ，即在表VIII的AMCL中添加CJ，我們的MCL進一步提高了船舶再識別的準確率，平均精度提高了1.5％，Rank-1分數提高了1.1％。我們進一步通過分別從AMCL中刪除RandomHorizontalFlip和RandomErasing來評估其性能，發現它們對於無監督式船舶再識別都是必要的。7）批量大小的影響：表IX顯示了批量大小對提出的MCL方法的影響。\n",
      "====================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "原文:\n",
      " The subscript\n",
      "values represent the numerical changes between the currentscores and the previous ones, where red and blue stand for\n",
      "an increase and a decrease respectively.It can be seen that\n",
      "our method benefits from larger batch size, which makes it\n",
      "possible to train vessel re-ID models with more samples in\n",
      "each iteration.By increasing the batch size from 32 to 256, the\n",
      "performance boosts significantly by 31.5% mAP and 35.5%\n",
      "Rank-1 score.This trend is mainly due to the absence of\n",
      "memory bank which usually provides enough negative samples\n",
      "for contrastive learning [54].We can also find that when the\n",
      "batch size becomes larger, the performance boost becomes not\n",
      "so significant.When the batch size is as large as 512, there is\n",
      "even a slight drop of performance.A possible reason is that\n",
      "with an excessively large batch size, a number of possible\n",
      "positives may be regarded as negatives at the starting epochs\n",
      "of training, which pushes the instances belonging to the same\n",
      "vessel identity away from each other and drives the model\n",
      "into a harder pattern to recover from such an undesired trend.\n",
      "====================\n",
      "翻譯結果:\n",
      " 下標值表示當前得分與以前得分之間的數值變化，其中紅色和藍色分別代表增加和減少。可以看出，我們的方法受益於更大的批量大小，這使得每次迭代可以使用更多樣本來訓練船舶重新識別模型。通過將批量大小從32增加到256，性能顯著提升31.5％的mAP和35.5％的Rank-1得分。這種趨勢主要是由於記憶庫的缺失，通常為對比學習提供足夠的負樣本[54]。我們還可以發現，當批量大小變得更大時，性能提升變得不那麼顯著。當批量大小達到512時，甚至會出現輕微的性能下降。可能的原因是，在過度大的批量大小下，許多可能的正樣本被視為負樣本，在訓練開始時會將屬於同一船舶身份識別的實例推離彼此，將模型推入較難從這種不希望的趨勢中恢復的模式中。\n",
      "====================\n",
      "====================\n",
      "原文:\n",
      " And once the distance between a positive pair is beyond the\n",
      "clustering threshold, they will never be clustered together in\n",
      "the subsequent epochs.VI.C ONCLUSION\n",
      "Unsupervised learning is closing the performance gap to\n",
      "supervised methods in many fields of computer vision.In this\n",
      "paper, we aim to deal with the unexplored hard positive prob-\n",
      "lemthat spreads positives apart in contrastive learning-based\n",
      "methods for unsupervised vessel re-ID.To mitigate the negative effects of the hard positive problem,\n",
      "we propose a multi-level contrastive learning (MCL) frame-\n",
      "work with no memory bank for vessel re-ID.On the one hand,\n",
      "we simply remove potential positives from the set of negative\n",
      "samples during the instance-level contrastive learning, which\n",
      "allows the re-ID model to learn detailed features of vessels\n",
      "more efficiently.On the other hand, the intra-batch cluster-\n",
      "level discrimination works as a complement for the instance-\n",
      "level discrimination, making the learning process more robust\n",
      "to noisy pseudo labels.\n",
      "====================\n",
      "翻譯結果:\n",
      " 一旦正對組之間的距離超過聚類閾值，它們將永遠不會在後續時期中被聚類在一起。第六部分 結論\n",
      "在許多計算機視覺領域中，無監督學習正在縮小與監督方法之間的性能差距。在本文中，我們旨在處理未經探索的硬正問題，該問題在基於對比學習的無監督血管再鑒定中將正面分散開來。為了減輕硬正問題的負面影響，我們提出了一種無記憶銀行的多級對比學習（MCL）框架，用於血管再識別。一方面，在實例級對比學習期間，我們簡單地將潛在正面從負樣本集合中移除，這使得重新ID模型更有效地學習船舶的詳細特徵。另一方面，批內集群級別判別作為實例級判別的補充，使學習過程對噪聲偽標籤更具韌性。\n",
      "====================\n",
      "====================\n",
      "原文:\n",
      " Furthermore, we build the VesselReID\n",
      "dataset for experimental studies, and the evaluation results on\n",
      "it demonstrate that the proposed MCL method outperforms the\n",
      "state-of-the-art methods in unsupervised vessel re-ID.We hope\n",
      "that both the proposed MCL method and the VesselReID\n",
      "dataset can be useful to inspire further research on unsuper-\n",
      "vised vessel re-ID.REFERENCES\n",
      "[1] P. Spagnolo, F. Filieri, C. Distante, P. L. Mazzeo, and P. D’Ambrosio,\n",
      "“A new annotated dataset for boat detection and re-identification,” in\n",
      "Proc.16th IEEE Int.Conf.Adv.Video Signal Based Surveill.(AVSS),\n",
      "Taipei, Taiwan, Sep. 2019, pp.1–7.This article has been accepted for inclusion in a future issue of this journal.Content is final as presented, with the exception of pagination.Authorized licensed use limited to: National Chung Hsing Univ.. Downloaded on March 27,2023 at 07:07:45 UTC from IEEE Xplore.Restrictions apply.\n",
      "====================\n",
      "翻譯結果:\n",
      " 此外，我們創建了VesselReID數據集供實驗研究使用，評估結果表明，所提出的MCL方法在非監督式船舶重新識別中優於現有最先進的方法。我們希望所提出的MCL方法和VesselReID數據集都能啟發進一步的非監督式船舶重新識別研究。參考文獻：[1] P. Spagnolo、F. Filieri、C. Distante、P. L. Mazzeo和P. D'Ambrosio，《A New Annotated Dataset for Boat Detection and Re-Identification》，《Proc.16th IEEE Int.Conf.Adv.Video Signal Based Surveill. (AVSS)》，臺北，臺灣，2019年9月，第1-7頁。本文已被接受包含在本期期刊中。內容已正式呈現，分頁除外。授權使用限於：國立中興大學。下載時間為2023年3月27日07:07:45 UTC，從IEEE Xplore下載。限制適用。\n",
      "====================\n",
      "====================\n",
      "原文:\n",
      " 12 IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS\n",
      "[2] A. Ghahremani, Y .Kong, E. Bondarev, and P. H. N. De With,\n",
      "“Re-identification of vessels with convolutional neural networks,” in\n",
      "Proc.5th Int.Conf.Comput.Technol.Appl., Istanbul, Turkey, Apr.2019,\n",
      "pp.93–97.[3] D. Qiao, G. Liu, F. Dong, S.-X.Jiang, and L. Dai, “Marine vessel re-\n",
      "identification: A large-scale dataset and global-and-local fusion-based\n",
      "discriminative feature learning,” IEEE Access, vol.8, pp.27744–27756,\n",
      "2020.[4] A. Ghahremani, T. Alkanat, E. Bondarev, and P. H. N. De With,\n",
      "“Maritime vessel re-identification: Novel VR-VCA dataset and a multi-\n",
      "branch architecture MVR-net,” Mach.Vis.Appl., vol.32, no.3, pp.1–14,\n",
      "May 2021.[5] H. G. J. Groot, M. H. Zwemer, E. Bondarev, R. G. J. Wijnhoven,\n",
      "and P. H. N. De With, “Tracklet-based vessel re-identification for multi-\n",
      "camera vessel-speed enforcement,” in Proc.Symp.Inf.Theory Signal\n",
      "Process., Eindhoven, The Netherlands, 2021, pp.132–138.[6] W. Li, X. Zhu, and S. Gong, “Harmonious attention network for\n",
      "person re-identification,” in Proc.\n",
      "====================\n",
      "翻譯結果:\n",
      " 智慧交通系統IEEE交易第12期\n",
      "[2] A. Ghahremani, Y. Kong, E. Bondarev,和P. H. N. De With，“使用卷積神經網絡重新識別船隻”，2019年4月土耳其伊斯坦布爾舉行的第5屆國際計算機技術應用大會論文集，頁93–97。[3] D. Qiao，G. Liu，F. Dong，S.-X. Jiang和L. Dai，“海洋船隻重新識別：一個大規模數據集和基於全局和局部融合的區分特徵學習”，IEEE Access，第8卷，頁27744-27756，2020年。[4] A. Ghahremani，T. Alkanat，E. Bondarev和P. H. N. De With，“海上船舶重新識別：新的VR-VCA數據集和多分支結構MVR-net”， Mach. Vis.Appl.，第32卷，第3期，頁1-14，2021年5月。[5] H. G. J. Groot，M. H. Zwemer，E. Bondarev，R. G. J. Wijnhoven和P. H. N. De With，“基於軌跡的船舶重新識別用於多攝像機船速執法”，2021年在荷蘭埃因霍芬舉行的Symposium on Information Theory and Signal Processing論文集，頁132–138。[6] W. Li，X. Zhu和S. Gong，“用於人員重新識別的和諧注意網絡”，在Proc中。\n",
      "====================\n",
      "====================\n",
      "原文:\n",
      " IEEE/CVF Conf.Comput.Vis.Pattern\n",
      "Recognit., Salt Lake City, Utah, Jun.2018, pp.2285–2294.[7] W. Zhang, X.He, W. Lu, H. Qiao, and Y .Li, “Feature aggregation\n",
      "with reinforcement learning for video-based person re-identification,”\n",
      "IEEE Trans.Neural Netw.Learn.Syst., vol.30, no.12, pp.3847–3852,\n",
      "Dec. 2019.[8] H. Yao, S. Zhang, R. Hong, Y .Zhang, C. Xu, and Q. Tian, “Deep\n",
      "representation learning with part loss for person re-identification,” IEEE\n",
      "Trans.Image Process., vol.28, no.6, pp.2860–2871, Jun.2019.[9] W. Zhang, X.He, X. Yu, W. Lu, Z. Zha, and Q. Tian, “A multi-scale\n",
      "spatial–temporal attention model for person re-identification in videos,”\n",
      "IEEE Trans.Image Process., vol.29, pp.3365–3373, 2020.[10] Z. Zhang, C. Lan, W. Zeng, X. Jin, and Z. Chen, “Relation-aware global\n",
      "attention for person re-identification,” in Proc.IEEE/CVF Conf.Comput.Vis.Pattern Recognit.(CVPR), Jun.2020, pp.3186–3195.[11] Z. Zhao, R. Song, Q. Zhang, P. Duan, and Y .Zhang, “JoT-GAN:\n",
      "A framework for jointly training GAN and person re-identification\n",
      "model,” ACM Trans.\n",
      "====================\n",
      "翻譯結果:\n",
      " IEEE/CVF Conf.Comput.Vis.Pattern Recognit.，犹他州盐湖城，2018年6月，第2285-2294页。[7] 张伟，何祥，陆伟弘，乔航和李焱，“基于强化学习的特征聚合方法在基于视频的人员重识别中的应用”，IEEE Trans.Neural Netw.Learn.Syst.，卷30，号12，页3847-3852，2019年12月。[8]姚豪，张硕硕，洪锐，张严强，徐超和田青：“深度表示学习在人员重识别中的应用”，IEEE Trans.Image Process.，卷28，号6，页2860-2871，2019年6月。[9] 张伟，何祥，于轩，陆伟弘，查正和田青：“一种基于多尺度空间-时间注意力模型的人员重识别方法”，IEEE Trans.Image Process.，卷29，页3365-3373，2020年。[10] 张振，兰聪，曾伟，金雪梅和陈铮：“基于关系感知全局注意的人员重识别方法”，在Proc.IEEE/CVF Conf.Comput.Vis.Pattern Recognit.(CVPR)上，2020年6月，第3186-3195页。[11] 赵志强，宋荣，张强，段鹏和张燕：“JoT-GAN：一种联合训练GAN和人员重识别模型的框架”，ACM Trans。\n",
      "====================\n",
      "====================\n",
      "原文:\n",
      " Multimedia Comput., Commun., Appl., vol.18,\n",
      "no.1, pp.1–18, Feb. 2022.[12] Y .Li, K. Liu, Y .Jin, T. Wang, and W. Lin, “V ARID: Viewpoint-aware\n",
      "re-identification of vehicle based on triplet loss,” IEEE Trans.Intell.Transp.Syst., vol.23, no.2, pp.1381–1390, Feb. 2020.[13] X. Chen, H. Sui, J. Fang, W. Feng, and M. Zhou, “Vehicle re-\n",
      "identification using distance-based global and partial multi-regional\n",
      "feature learning,” IEEE Trans.Intell.Transp.Syst., vol.22, no.2,\n",
      "pp.1276–1286, Feb. 2021.[14] F. Shen, J. Zhu, X. Zhu, Y .Xie, and J. Huang, “Exploring spatial signifi-\n",
      "cance via hybrid pyramidal graph network for vehicle re-identification,”\n",
      "IEEE Trans.Intell.Transp.Syst., vol.23, no.7, pp.8793–8804,\n",
      "Jul.2022.[15] Y .Lin, X. Dong, L. Zheng, Y .Yan, and Y .Yang, “A bottom-up clustering\n",
      "approach to unsupervised person re-identification,” in Proc.AAAI Conf.Artif.Intell., Honolulu, Hawaii, 2019, pp.8738–8745.[16] D. Wang and S. Zhang, “Unsupervised person re-identification via multi-\n",
      "label classification,” in Proc.\n",
      "====================\n",
      "翻譯結果:\n",
      " 多媒體計算、通信和應用，第18卷，第1期，2022年2月，頁1-18。[12]李毅，劉凱，金洋，王腾和林威，\"基于三元组损失的视角感知车辆再识别V ARID\"，IEEE智能交通系统学报，第23卷，第2期，2020年2月，頁1381-1390。[13]陈欣，隋红，方军，冯威和周敏，\"基于距离的全局和局部多区域特征学习的车辆再识别\"，IEEE智能交通系统学报，第22卷，第2期，2021年2月，頁1276-1286。[14]沈峰，朱杰，朱旭，谢远征和黄健，\"基于混合金字塔图网络探索空间显著性的车辆再识别\"，IEEE智能交通系统学报，第23卷，第7期，2022年7月，頁8793-8804。[15]林勇，董旭，郑林，颜勇和杨洋，\"一种自下而上的聚类方法用于非监督人员再识别\"，于AAAI人工智能会议论文集，夏威夷檀香山，2019年，頁8738-8745。[16]王丹和张帅，\"基于多标签分类的非监督人员再识别\"，于Proc.\n",
      "====================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "警告呼叫API失敗\n",
      "====================\n",
      "原文:\n",
      " Intell.Transp.Syst., vol.23, no.8, pp.11422–11435, Aug. 2022.[22] J. Yu and H. Oh, “Unsupervised vehicle re-identification via self-\n",
      "supervised metric learning using feature dictionary,” in Proc.IEEE/RSJ\n",
      "Int.Conf.Intell.Robots Syst.(IROS), Prague, Czech Republic,\n",
      "Sep. 2021, pp.3806–3813.[23] Y .Lin, Y .Wu, C. Yan, M. Xu, and Y .Yang, “Unsupervised person\n",
      "re-identification via cross-camera similarity exploration,” IEEE Trans.Image Process., vol.29, pp.5481–5490, 2020.[24] L. Zheng, L. Shen, L. Tian, S. Wang, J. Wang, and Q. Tian, “Scal-\n",
      "able person re-identification: A benchmark,” in Proc.IEEE Int.Conf.Comput.Vis.(ICCV), Santiago, Chile, Dec. 2015, pp.1116–1124.[25] X. Liu, W. Liu, T. Mei, and H. Ma, “PROVID: Progressive and\n",
      "multimodal vehicle reidentification for large-scale urban surveillance,”\n",
      "IEEE Trans.Multimedia, vol.20, no.3, pp.645–658, Mar.2017.[26] T. Wang and P. Isola, “Understanding contrastive representation learning\n",
      "through alignment and uniformity on the hypersphere,” in Proc.\n",
      "====================\n",
      "翻譯結果:\n",
      " 智能運輸系統，第23卷，第8期，2022年8月，頁11422-11435 [22] J. Yu和H. Oh，“通過自我監督度量學習使用特徵字典進行無監督車輛再識別”，發表於2021年9月IEEE/RSJ智能機器人系統國際會議（IROS），捷克布拉格，頁3806-3813 [23] Y.Lin，Y.Wu，C.Yan，M.Xu和Y.Yang，“通過跨攝像機相似度探索進行無監督人再識別”，IEEE圖像處理，第29卷，頁5481-5490，2020 [24] L.Zheng，L.Shen，L.Tian，S.Wang，J.Wang和Q.Tian，“可擴展的人再識別：基準測試”，發表於2015年12月IEEE計算機視覺國際會議（ICCV），智利聖地亞哥，頁1116-1124 [25] X.Liu，W.Liu，T.Mei和H.Ma，“PROVID：大規模城市監視的漸進式多模式車輛再識別”，IEEE多媒體，第20卷，第3期，頁645-658，2017年3月。 [26] T.Wang和P.Isola，“通過超球面上的對齊和一致性理解對比表示學習”，發表於XXX。\n",
      "====================\n",
      "====================\n",
      "原文:\n",
      " Int.Conf.Mach.Learn., 2020, pp.9871–9881.[27] H. Gao, J. Xiao, Y .Yin, T. Liu, and J. Shi, “A mutually supervised\n",
      "graph attention network for few-shot segmentation: The perspective of\n",
      "fully utilizing limited samples,” IEEE Trans.Neural Netw.Learn.Syst.,\n",
      "early access, Mar.14, 2022, doi: 10.1109/TNNLS.2022.3155486.[28] M. Zhuge, D.-P.Fan, N. Liu, D. Zhang, D. Xu, and L. Shao,\n",
      "“Salient object detection via integrity learning,” IEEE Trans.Pattern\n",
      "Anal.Mach.Intell., vol.45, no.3, pp.3738–3752, Mar.2023, doi:\n",
      "10.1109/TPAMI.2022.3179526.[29] T. Meinhardt, A. Kirillov, L. Leal-Taixe, and C. Feichtenhofer, “Track-\n",
      "Former: Multi-object tracking with transformers,” in Proc.IEEE/CVF\n",
      "Conf.Comput.Vis.Pattern Recognit.(CVPR), New Orleans, Louisiana,\n",
      "Jun.2022, pp.8844–8854.[30] H. Gao, B. Qiu, R. J. D. Barroso, W. Hussain, Y .Xu, and\n",
      "X. Wang, “TSMAE: A novel anomaly detection approach for Internet\n",
      "of Things time series data using memory-augmented autoencoder,”\n",
      "IEEE Trans.Netw.Sci.Eng., early access, Mar.\n",
      "====================\n",
      "翻譯結果:\n",
      " 2022，doi: 10.1109/TNSE.2022.3183418。\n",
      "\n",
      "[27] H. Gao、J. Xiao、Y. Yin、T. Liu和J. Shi，“一個相互監督的圖注意力網絡用於少量樣本分割：充分利用有限樣本的觀點”，IEEE Trans. Neural Netw. Learn. Syst.，2022年3月14日，提前發布，doi：10.1109/TNNLS.2022.3155486。\n",
      "\n",
      "[28] M. Zhuge、D.-P.Fan、N. Liu、D. Zhang、D. Xu和L. Shao，“基於完整性學習的突出物檢測”，IEEE Trans. Pattern Anal. Mach. Intell.，2023年3月，第45卷，第3期，頁3738-3752，doi：10.1109/TPAMI.2022.3179526。\n",
      "\n",
      "[29] T. Meinhardt、A. Kirillov、L. Leal-Taixe和C. Feichtenhofer，“Track-Former：使用Transformer進行多目標跟踪”，在2022年IEEE/CVF計算機視覺和模式識別（CVPR）會議中，路易斯安那州新奧爾良，2022年6月，頁8844-8854。\n",
      "\n",
      "[30] H. Gao、B. Qiu、R. J. D. Barroso、W. Hussain、Y. Xu和X. Wang，“TSMAE：一種使用記憶增強自編碼器的物聯網時間序列數據新異常檢測方法”，IEEE Trans. Netw. Sci. Eng.，2022年3月提前發布，doi：10.1109/TNSE.2022.3183418。\n",
      "====================\n",
      "====================\n",
      "原文:\n",
      " 29, 2022, doi:\n",
      "10.1109/TNSE.2022.3163144.[31] X. Zhou, Y .Li, and W. Liang, “CNN-RNN based intelligent recom-\n",
      "mendation for online medical pre-diagnosis support,” IEEE/ACM Trans.Comput.Biol.Bioinf., vol.18, no.3, pp.912–921, May 2021.[32] X. Zhou, W. Liang, W. Li, K. Yan, S. Shimizu, and K. I. Wang,\n",
      "“Hierarchical adversarial attacks against graph neural network based IoT\n",
      "network intrusion detection system,” IEEE Internet Things J., vol.9,\n",
      "no.12, pp.9310–9319, Jun.2022.[33] Z. Zhong, L. Zheng, Z. Luo, S. Li, and Y .Yang, “Invariance matters:\n",
      "Exemplar memory for domain adaptive person re-identification,” in Proc.IEEE/CVF Conf.Comput.Vis.Pattern Recognit.(CVPR) , Long Beach,\n",
      "CA, USA, Jun.2019, pp.598–607.[34] H.-X.Yu, W.-S. Zheng, A. Wu, X. Guo, S. Gong, and J.-H. Lai,\n",
      "“Unsupervised person re-identification by soft multilabel learning,” in\n",
      "Proc.IEEE/CVF Conf.Comput.Vis.Pattern Recognit.(CVPR), Long\n",
      "Beach, CA, USA, Jun.2019, pp.2148–2157.[35] H. Fan, L. Zheng, C. Yan, and Y .Yang, “Unsupervised person re-\n",
      "identification: Clustering and fine-tuning,” ACM Trans.\n",
      "====================\n",
      "翻譯結果:\n",
      " 計算機生物學與生物信息學，卷18，第3期，2021年5月，頁912-921。\n",
      "\n",
      "[32] X. Zhou、W. Liang、W. Li、K. Yan、S. Shimizu 和 K. I. Wang，\"基於層次式對抗性攻擊的圖神經網絡IoT網絡入侵檢測系統\", IEEE物聯網雜誌，卷9, 第12期，2022年6月，頁9310-9319。\n",
      "\n",
      "[33] Z. Zhong、L. Zheng、Z. Luo、S. Li和Y .Yang，\"不變性很重要: 基於範例記憶的領域自適應人員重識別\", 在2019 IEEE/CVF計算機視覺和模式識別會議（CVPR）中，美國加利福尼亞州長灘市，2019年6月，頁598-607。\n",
      "\n",
      "[34] H.-X.Yu、W.-S. Zheng、A. Wu、X. Guo、S. Gong和J.-H. Lai，\"軟多標簽學習的無監督人員重識別\", 在2019 IEEE/CVF計算機視覺和模式識別會議（CVPR），美國加利福尼亞州長灘市，2019年6月，頁2148-2157。\n",
      "\n",
      "[35] H. Fan、L. Zheng、C. Yan和Y .Yang，\"無監督人員重識別: 聚類和微調\", ACM Trans.計算機圖形學，卷38，第6期，2019年12月， 文章11。\n",
      "====================\n",
      "====================\n",
      "原文:\n",
      " Multimedia\n",
      "Comput., Commun., Appl., vol.14, no.4, pp.1–18, 2018.[36] Y .Ge, D. Chen, and H. Li, “Mutual mean-teaching: Pseudo label refinery\n",
      "for unsupervised domain adaptation on person re-identification,” in Proc.Int.Conf.Learn.Represent., 2020, pp.1–15.[37] F. Zhao, S. Liao, G. Xie, J. Zhao, K. Zhang, and L. Shao, “Unsupervised\n",
      "domain adaptation with noise resistible mutual-training for person re-\n",
      "identification,” in Proc.Eur.Conf.Comput.Vis., Glasgow, U.K., 2020,\n",
      "pp.526–544.[38] R. M. S. Bashir, M. Shahzad, and M. M. Fraz, “VR-PROUD: Vehicle re-\n",
      "identification using progressive unsupervised deep architecture,” Pattern\n",
      "Recognit., vol.90, pp.52–65, Jun.2019.[39] H. Wang, J. Peng, G. Jiang, and X. Fu, “Learning multiple semantic\n",
      "knowledge for cross-domain unsupervised vehicle re-identification,” in\n",
      "Proc.IEEE Int.Conf.Multimedia Expo (ICME), Shenzhen, China,\n",
      "Jul.2021, pp.1–6.[40] Y .Wu, Y .Lin, X. Dong, Y .Yan, W. Ouyang, and Y .Yang, “Exploit\n",
      "the unknown gradually: One-shot video-based person re-identification\n",
      "by stepwise learning,” in Proc.\n",
      "====================\n",
      "翻譯結果:\n",
      " 多媒體\n",
      "計算機、通信、應用，第14卷，第4期，2018年，頁1-18。[36] Y .Ge、D. Chen和H. Li，“互惠教學：偽標籤精煉用於無監督領域適應中的人物重新識別”，在2020年學習代表國際會議上，第1-15頁。[37] F. Zhao、S. Liao、G. Xie、J. Zhao、K. Zhang和L. Shao，“帶噪聲的互惠訓練無監督領域適應，用於人物重新識別”，在2020年歐洲計算機視覺會議上，格拉斯哥，英國，第526-544頁。[38] R. M. S. Bashir、M. Shahzad和M. M. Fraz，“VR-PROUD：使用漸進式無監督深度架構的車輛重新識別”，模式識別，第90卷，第52-65頁，2019年6月。[39] H. Wang、J. Peng、G. Jiang和X. Fu，“學習多重語義知識進行跨域無監督車輛重新識別”，在2021年IEEE國際多媒體展覽會（ICME）上，中國深圳，第1-6頁。[40] Y .Wu、Y .Lin、X. Dong、Y .Yan、W. Ouyang和Y .Yang，“逐步利用未知：通過逐步學習的單次基於視頻的人物重新識別”，在会議中。\n",
      "====================\n",
      "====================\n",
      "原文:\n",
      " IEEE/CVF Conf.Comput.Vis.Pattern\n",
      "Recognit., Salt Lake City, Utah, Jun.2018, pp.5177–5186.[41] Y .Wu, Y .Lin, X. Dong, Y .Yan, W. Bian, and Y .Yang, “Progressive\n",
      "learning for person re-identification with one example,” IEEE Trans.Image Process., vol.28, no.6, pp.2872–2881, Jun.2019.[42] Y .Lin, L. Xie, Y .Wu, C. Yan, and Q. Tian, “Unsupervised person re-\n",
      "identification via softened similarity learning,” in Proc.IEEE/CVF Conf.Comput.Vis.Pattern Recognit.(CVPR), Jun.2020, pp.3390–3399.[43] Y .Zheng et al., “Online pseudo label generation by hierarchical cluster\n",
      "dynamics for adaptive person re-identification,” in Proc.IEEE/CVF Int.Conf.Comput.Vis.(ICCV), Oct. 2021, pp.8371–8381.This article has been accepted for inclusion in a future issue of this journal.Content is final as presented, with the exception of pagination.Authorized licensed use limited to: National Chung Hsing Univ.. Downloaded on March 27,2023 at 07:07:45 UTC from IEEE Xplore.Restrictions apply.\n",
      "====================\n",
      "翻譯結果:\n",
      " \"IEEE/CVF Conf.Comput.Vis.Pattern Recognit.\" Salt Lake City, Utah, Jun. 2018, pp. 5177-5186。[41] Y. Wu、Y. Lin、X. Dong、Y. Yan、W. Bian和Y. Yang，“一個範例的漸進式學習用於人員重新辨識”，IEEE Trans.Image Process.，vol. 28，no. 6，pp. 2872-2881，Jun. 2019。[42] Y. Lin、L. Xie、Y. Wu、C. Yan和Q. Tian，“藉由軟化相似性學習進行無監督式人員重新辨識”，於2019年IEEE/CVF Conf.Comput.Vis.Pattern Recognit.(CVPR)中，Jun. 2020，pp.3390-3399。[43] Y. Zheng等人，“適應性人員重新辨識的階層式集群動態線上虛標籤生成”，於2021年IEEE/CVF國際計算機視覺會議(ICCV)中，Oct. 2021, pp.8371-8381。本文已被接受包含於此期刊的未來議題內。資料內容皆為最終呈現狀態，唯獨分頁編號除外。正式授權使用僅限國立中興大學。從IEEE Xplore 於2023年3月27日07:07:45 UTC下載，適用限制。\n",
      "====================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "原文:\n",
      " ZHANG et al.: UNSUPERVISED MARITIME VESSEL RE-IDENTIFICATION WITH MCL 13\n",
      "[44] Y .Cho, W. J. Kim, S. Hong, and S.-E. Yoon, “Part-based pseudo\n",
      "label refinement for unsupervised person re-identification,” in Proc.IEEE/CVF Conf.Comput.Vis.Pattern Recognit.(CVPR), New Orleans,\n",
      "Louisiana, Jun.2022, pp.7308–7318.[45] X. Zhang, Y .Ge, Y .Qiao, and H. Li, “Refining pseudo labels\n",
      "with clustering consensus over generations for unsupervised object re-\n",
      "identification,” in Proc.IEEE/CVF Conf.Comput.Vis.Pattern Recognit.(CVPR), Jun.2021, pp.3436–3445.[46] T. He, L. Shen, Y .Guo, G. Ding, and Z. Guo, “Secret: Self-consistent\n",
      "pseudo label refinement for unsupervised domain adaptive person re-\n",
      "identification,” in Proc.AAAI Conf.Artif.Intell., 2022, pp.879–887.[47] S. Xuan and S. Zhang, “Intra-inter camera similarity for unsupervised\n",
      "person re-identification,” in Proc.IEEE/CVF Conf.Comput.Vis.Pattern\n",
      "Recognit.(CVPR), Jun.2021, pp.11926–11935.[48] F. Yang et al., “Joint noise-tolerant learning and meta camera\n",
      "shift adaptation for unsupervised person re-identification,” in Proc.\n",
      "====================\n",
      "翻譯結果:\n",
      " 無監督海上船舶重新識別使用MCL 13的ZHANG等人:\n",
      "[44] Y.Cho，W.J.Kim，S.Hong和S.-E. Yoon，“無監督人員重新識別的基於部件的偽標籤精化”，於IEEE/CVF Conf.Comput.Vis.Pattern Recognit.(CVPR)中，新奧爾良，路易斯安那州，2022年6月，頁7308-7318。[45] X. Zhang，Y.Ge，Y.Qiao和H.Li，“通過世代聚類共識精化偽標籤的無監督對象重新識別”，於IEEE/CVF Conf.Comput.Vis.Pattern Recognit.(CVPR)中，2021年6月，頁3436-3445。[46] T. He，L.Shen，Y. Guo，G.Ding和Z. Guo，“秘密：自一致的偽標籤精化用於無監督領域自適應人員重新識別”，於AAAI Conf.Artif.Intell.中，2022年，頁879-887。[47] S. Xuan和S. Zhang，“基於內部 - 外部相機相似性的無監督人員重新識別”，於IEEE/CVF Conf.Comput.Vis.Pattern Recognit.(CVPR)中，2021年6月，頁11926-11935。[48] F. Yang等人，“聯合抗噪學習和元相機位移調適的無監督人員重新識別”，在Proc.中。\n",
      "====================\n",
      "====================\n",
      "原文:\n",
      " IEEE/CVF Conf.Comput.Vis.Pattern Recognit.(CVPR), Jun.2021,\n",
      "pp.4855–4864.[49] A. Ghahremani, Y .Kong, E. Bondarev, and P. H. N. De With, “Towards\n",
      "parameter-optimized vessel re-identification based on IORNet,” in Proc.Int.Conf.Comput.Sci., Faro, Portugal, 2019, 125–136.[50] X. Han, Z. Chen, R. Wang, and P. Zhao, “Joint label refinement and\n",
      "contrastive learning with hybrid memory for unsupervised marine object\n",
      "re-identification,” in Proc.ACM Multimedia Asia, Dec. 2021, pp.1–6.[51] H. G. J. Groot, M. H. Zwemer, R. G. J. Wijnhoven, Y .Bondarev,\n",
      "and P. H. N. De With, “Vessel-speed enforcement system by multi-\n",
      "camera detection and re-identification,” in Proc.15th Int.Joint Conf.Comput.Vis., Imag.Comput.Graph.Theory Appl., Valletta, Malta, 2020,\n",
      "pp.268–277.[52] M. H. Zwemer, H. G. J. Groot, R. Wijnhoven, E. Bondarev, and\n",
      "P. H. N. De With, “Multi-camera vessel-speed enforcement by enhanc-\n",
      "ing detection and re-identification techniques,” Sensors, vol.21, no.14,\n",
      "p. 4659, Jul.2021.[53] K. He, H. Fan, Y .\n",
      "====================\n",
      "翻譯結果:\n",
      " 2021年6月IEEE/CVF计算机视觉模式识别会议(CVPR)，第4855-4864页。[49]A. Ghahremani, Y.Kong, E.Bondarev和P.H.N. De With，“基于IORNet的参数优化血管再识别研究”，发表于2019年计算科学国际会议，葡萄牙法鲁，125–136页。[50]X. Han，Z. Chen，R. Wang和P. Zhao，“基于混合内存的联合标签细化和对比学习用于无监督海洋物体再识别”，发表于2021年ACM Multimedia Asia会议，第1-6页。[51]H.G.J. Groot，M.H. Zwemer，R.G.J. Wijnhoven，Y.Bondarev和P.H.N. De With，“多摄像头检测和再识别的船只速度执法系统”，发表于2020年第15届国际联合计算机视觉、图像计算和图形理论应用会议，马耳他瓦莱塔，第268-277页。[52]M.H. Zwemer，H.G.J. Groot，R. Wijnhoven，E. Bondarev和P.H.N. De With，“通过增强检测和再识别技术实现多摄像头船只速度执法”，发表于2021年7月的Sensors杂志，第21卷，第14期，第4659页。[53]K. He，H. Fan，Y.\n",
      "====================\n",
      "====================\n",
      "原文:\n",
      " Wu, S. Xie, and R. Girshick, “Momentum contrast for\n",
      "unsupervised visual representation learning,” in Proc.IEEE/CVF Conf.Comput.Vis.Pattern Recognit.(CVPR), Jun.2020, pp.9729–9738.[54] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton, “A simple framework\n",
      "for contrastive learning of visual representations,” in Proc.Int.Conf.Machin.Learn., 2020, pp.1597–1607.[55] J.B.Grill et al., “Bootstrap your own latent—A new approach to self-\n",
      "supervised learning,” in Proc.Adv.Neural Inf.Process.Syst., 2020,\n",
      "pp.21271–21284.[56] Y .Wu, L. Zhu, Y .Yan, and Y .Yang, “Dual attention matching for audio-\n",
      "visual event localization,” in Proc.IEEE/CVF Int.Conf.Comput.Vis.(ICCV), Seoul, (South) Korea, Oct. 2019, pp.6292–6300.[57] J. Cheng et al., “Foreground object structure transfer for unsupervised\n",
      "domain adaptation,” 2021, arXiv:2109.06543.[58] Q. Chen and J. Zhang, “Multi-level contrastive learning for few-shot\n",
      "problems,” 2021, arXiv:2107.07608.[59] B. Chen, W. Guo, B. Gu, Q. Liu, and Y .Wang, “Multi-level contrastive\n",
      "learning for cross-lingual alignment,” in Proc.\n",
      "====================\n",
      "翻譯結果:\n",
      " IEEE/CVF CVPR 計算機視覺與模式識別會議，2020年6月，Wu、S. Xie和R. Girshick發表了「動量對比的非監督視覺表示學習」技術文獻，頁碼在9729-9738頁。[54] T. Chen、S. Kornblith、M. Norouzi和G. Hinton在2020年的「視覺表示對比學習的簡單框架」技術文獻中，發表了對比學習的方法，在國際機器學習大會上的發表者論文集中，頁碼在1597-1607頁。[55] J.B.Grill等人在2020年的「Bootstrap your own latent—自監督學習的新方法」技術文獻中，發表了自監督學習的新方法，在神經信息處理系統的進階論文集中，頁碼在21271-21284頁。[56] Y .Wu、L. Zhu、Y .Yan和Y .Yang在2019年的「聲音-視覺事件定位的雙重注意匹配」技術文獻中，發表了雙重注意匹配的方法，在IEEE/CVF計算機視覺國際會議（ICCV）中發表，會議地點為南韓首爾，頁碼在6292-6300頁。[57] J. Cheng等人在2021年的「前景對象結構轉移的非監督領域適應」技術文獻中，發表了前景對象結構轉移的方法，在arXiv上發表論文，編號為2109.06543。[58] Q. Chen和J. Zhang在2021年的「多層對比學習用於少樣本問題」技術文獻中，發表了多層對比學習的方法，在arXiv上發表論文，編號為2107.07608。[59] B. Chen、W. Guo、B. Gu、Q. Liu和Y .Wang在2021年的「多層對比學習用於跨語言對齊」技術文獻中，發表了多層對比學習的方法，在IEEE/CVF CVPR 計算機視覺與模式識別會議的論文集中。\n",
      "====================\n",
      "====================\n",
      "原文:\n",
      " IEEE Int.Conf.Acoust.,\n",
      "Speech Signal Process.(ICASSP), May 2022, pp.7947–7951.[60] X. Li, P. Li, Y .Wang, X. Liu, and W. Lam, “Enhancing dialogue gen-\n",
      "eration via multi-level contrastive learning,” 2020, arXiv:2009.09147.[61] P. Shao, T. Liu, D. Zhang, J. Tao, F. Che, and G. Yang, “Multi-level\n",
      "graph contrastive learning,” 2021, arXiv:2107.02639.[62] A.Van Den Oord, Y .Li, and O. Vinyals, “Representation learning with\n",
      "contrastive predictive coding,” 2018, arXiv:1807.03748.[63] P. Khosla et al., “Supervised contrastive learning,” in Proc.Adv.Neural\n",
      "Inf.Process.Syst., 2020, pp.18661–18673.[64] S. Ioffe and C. Szegedy, “Batch normalization: Accelerating deep\n",
      "network training by reducing internal covariate shift,” in Proc.32nd\n",
      "Int.Conf.Mach.Learn., Lille, France, vol.37, 2015, pp.448–456.[65] M. Ester, H. Kriegel, J. Sander, and X. Xu, “A density-based algorithm\n",
      "for discovering clusters in large spatial databases with noise,” in Proc.2nd Int.Conf.Knowl.Discov.Data Mining, Portland, Oregon, 1996,\n",
      "pp.\n",
      "====================\n",
      "翻譯結果:\n",
      " IEEE國際聲學、語音和信號處理會議（ICASSP），2022年5月，第7947至7951頁。[60] X. Li、P. Li、Y. Wang、X. Liu和W. Lam，“通過多級對比學習增強對話生成”，2020年，arXiv:2009.09147。[61] P. Shao、T. Liu、D. Zhang、J. Tao、F. Che和G. Yang，“多級圖形對比學習”，2021年，arXiv:2107.02639。[62] A. Van Den Oord、Y. Li和O. Vinyals，“對比預測編碼的表示學習”，2018年，arXiv:1807.03748。[63] P. Khosla等人，“監督對比學習”，於Adv.Neural Inf.Process.Syst.的Proc中，2020年，第18661至18673頁。[64] S. Ioffe和C. Szegedy，“批量標準化：通過減少內部協變量移位來加速深度網絡訓練”，在第32屆國際機器學習會議中，法國里爾，第37卷，2015年，第448至456頁。[65] M. Ester、H. Kriegel、J. Sander和X. Xu，“一種基於密度的算法，用於在帶有噪聲的大型空間數據庫中發現集群”，在第2屆知識發現和數據挖掘國際會議中，俄勒岡州波特蘭，1996年，第頁。\n",
      "====================\n",
      "====================\n",
      "原文:\n",
      " 226–231.[66] Z. Zhong, L. Zheng, D. Cao, and S. Li, “Re-ranking person re-\n",
      "identification with K-reciprocal encoding,” in Proc.IEEE Conf.Com-\n",
      "put.Vis.Pattern Recognit.(CVPR), Honolulu, Hawaii, Jul.2017,\n",
      "pp.1318–1327.[67] Z. Zhong, L. Zheng, G. Kang, S. Li, and Y .Yang, “Random erasing\n",
      "data augmentation,” in Proc.AAAI Conf.Artif.Intell., New York, NY ,\n",
      "USA, 2020, pp.13001–13008.[68] L. Van Der Maaten and G. Hinton, “Visualizing data using t-SNE,”\n",
      "J. Mach.Learn.Res., vol.9, pp.2579–2605, Nov. 2008.[69] Z. Zhong, L. Zheng, Z. Zheng, S. Li, and Y .Yang, “Camera style adap-\n",
      "tation for person re-identification,” in Proc.IEEE/CVF Conf.Comput.Vis.Pattern Recognit., Salt Lake City, Utah, Jun.2018, pp.5157–5166.Qian Zhang received the B.Eng.degree from North-\n",
      "eastern University at Qinhuangdao, Qinhuangdao,\n",
      "China, in 2018, and the M.S.degree from Shandong\n",
      "University, Jinan, China, in 2021.She is currently\n",
      "pursuing the Ph.D. degree in pattern recognition and\n",
      "intelligent systems.Her research interests include computer vision and\n",
      "machine learning.\n",
      "====================\n",
      "翻譯結果:\n",
      " 226-231 [66] 鐘政，鄭林，曹東，李森，“使用K-reciprocal編碼重新排列人物識別，”於IEEE Conf.Com-put.Vis.Pattern Recognit. (CVPR)，夏威夷檀香山，2017 年7 月，頁1318-1327。[67] 鐘政，鄭林，康國明，李森，楊洋，“隨機擦除數據增強”，於AAAI Conf.Artif.Intell.，紐約，NY, USA，2020年，頁13001-13008。[68] 馬坦.范德和G.辛頓，“使用t-SNE對數據進行可視化”，《機器學習研究》，第9卷，第11期，頁2579-2605，2008年11月。[69] 鐘政，鄭林，鄭政，李森，楊洋，“用於人物再識別的攝像機風格適應”，於IEEE/CVF Conf.Comput.Vis.Pattern Recognit. ，犹他州盐湖城，2018年6月，頁5157-5166。 張謙在2018年獲得東北大學秦皇島分校的學士學位，並在2021年獲得山東大學碩士學位。她目前正在攻讀圖像識別和智能系統的博士學位。她的研究興趣包括計算機視覺和機器學習。\n",
      "====================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "原文:\n",
      " Mingxin Zhang (Graduate Student Member, IEEE)\n",
      "is currently pursuing the Ph.D. degree in pattern\n",
      "recognition and intelligent systems.His research interests include computer vision,\n",
      "robot navigation, and deep learning.Jinghe Liu received the B.Eng.and M.S.degrees\n",
      "from the School of Control Science and Engineering,\n",
      "Shandong University, Jinan, China, in 2019 and\n",
      "2022, respectively.His research interests include computer vision and\n",
      "intelligent systems.Xuanyu He received the B.Eng.degree from Zhe-\n",
      "jiang University, Hangzhou, China, in 2014, and the\n",
      "Ph.D. degree from the School of Control Science and\n",
      "Engineering, Shandong University, China, in 2021.His current research interests include deep learn-\n",
      "ing, computer vision, and pattern recognition.Ran Song (Member, IEEE) received the B.Eng.degree in telecommunications engineering from\n",
      "Shandong University, China, in 2005, and the Ph.D.\n",
      "degree in computer vision from the University of\n",
      "York, U.K., in 2009.He is currently a Professor with the School of\n",
      "Control Science and Engineering, Shandong Uni-\n",
      "versity.\n",
      "====================\n",
      "翻譯結果:\n",
      " 張明新（IEEE研究生會員）目前正在攻讀模式識別和智能系統的博士學位。他的研究興趣包括計算機視覺、機器人導航和深度學習。劉景和於2019年和2022年分別從山東大學控制科學與工程學院獲得學士和碩士學位。他的研究興趣包括計算機視覺和智能系統。何宣宇於2014年獲得浙江大學工學學士學位，並於2021年從山東大學控制科學與工程學院獲得博士學位。他目前的研究興趣包括深度學習、計算機視覺和模式識別。宋然（IEEE會員）於2005年從山東大學獲得電信工程學士學位，並於2009年從英國約克大學獲得計算機視覺博士學位。他現在是山東大學控制科學與工程學院的教授。\n",
      "====================\n",
      "====================\n",
      "原文:\n",
      " He has published more than 80 papers\n",
      "in peer-reviewed journals and international confer-\n",
      "ences.His research interests include 3D shape anal-\n",
      "ysis and 3D visual perception.Wei Zhang (Member, IEEE) received the Ph.D.\n",
      "degree in electronic engineering from The Chinese\n",
      "University of Hong Kong in 2010.He is currently with the School of Control Science\n",
      "and Engineering, Shandong University, Jinan, China.His research interests include computer vision and\n",
      "robotics.Dr. Zhang has served as a program committee\n",
      "member and a reviewer for various international\n",
      "conferences and journals.This article has been accepted for inclusion in a future issue of this journal.Content is final as presented, with the exception of pagination.Authorized licensed use limited to: National Chung Hsing Univ.. Downloaded on March 27,2023 at 07:07:45 UTC from IEEE Xplore.Restrictions apply.\n",
      "====================\n",
      "翻譯結果:\n",
      " 他已經在同行評審期刊和國際會議上發表了80多篇論文，他的研究方向包括三維形狀分析和三維視覺知覺。韋張（IEEE會員）於2010年從香港中文大學電子工程獲得博士學位，現任職於中國濟南山東大學控制科學與工程學院。他的研究方向包括計算機視覺和機器人技術。韋博士曾擔任多個國際會議和期刊的程序委員和審稿人。本文已獲接受並將刊登在未來的期刊內，內容以所提供的最終版為準，除了頁碼之外。本文授權的使用範圍僅限國立中興大學，下載時間為2023年3月27日07:07:45 UTC，使用時須遵守相關限制。\n",
      "====================\n",
      "程式運行時間：2079.65秒\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from pypdf import PdfReader\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "import openai\n",
    "import pandas as pd\n",
    "start_time = time.time()\n",
    "openai.api_key = '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'\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "pdf_name = \"Unsupervised_Maritime_Vessel_Re-Identification_With_Multi-Level_Contrastive_Learning.pdf\"\n",
    "reader = PdfReader(pdf_name)\n",
    "number_of_pages = len(reader.pages)\n",
    "\n",
    "chunks = []\n",
    "\n",
    "\n",
    "translation = pd.DataFrame(columns=['原文', '翻譯結果'])\n",
    "\n",
    "for i in range(number_of_pages):\n",
    "    page = reader.pages[i]\n",
    "    text = page.extract_text()\n",
    "    sentences = sent_tokenize(text)\n",
    "    input_sentences = ''\n",
    "  \n",
    "    for sentence in sentences:\n",
    "        input_sentences += sentence\n",
    "        if len(input_sentences) > 1000:\n",
    "            chunks.append(input_sentences)\n",
    "            input_sentences = ''\n",
    "    chunks.append(input_sentences)\n",
    "    \n",
    "\n",
    "        \n",
    "for i in range(len(chunks)):\n",
    "    try:\n",
    "        completion = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"請你成為文章翻譯的小幫手，請協助翻譯以下技術文件，以繁體中文輸出\"},\n",
    "                    {\"role\": \"user\", \"content\": chunks[i]},\n",
    "                  ]\n",
    "                                                  )\n",
    "        print('='*20) \n",
    "        print('原文:\\n', chunks[i])\n",
    "        print('='*20)  \n",
    "        print('翻譯結果:\\n', completion.choices[0].message.content)\n",
    "        print('='*20)\n",
    "        translation.loc[i] = [chunks[i], completion.choices[0].message.content]\n",
    "    except:\n",
    "        print(\"警告呼叫API失敗\")\n",
    "\n",
    "translation.to_csv(pdf_name+\"translation\"+\".csv\", encoding='utf-8-sig')\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "# 計算程式運行時間\n",
    "duration = end_time - start_time\n",
    "print(f\"程式運行時間：{duration:.2f}秒\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "408495c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "translation.to_csv(pdf_name+\"translation\"+\".txt\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cac559e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\William\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "警告呼叫API失敗\n",
      "警告呼叫API失敗\n",
      "警告呼叫API失敗\n",
      "警告呼叫API失敗\n",
      "警告呼叫API失敗\n",
      "警告呼叫API失敗\n",
      "警告呼叫API失敗\n",
      "警告呼叫API失敗\n",
      "警告呼叫API失敗\n",
      "警告呼叫API失敗\n",
      "警告呼叫API失敗\n",
      "警告呼叫API失敗\n",
      "警告呼叫API失敗\n",
      "警告呼叫API失敗\n",
      "警告呼叫API失敗\n",
      "警告呼叫API失敗\n",
      "警告呼叫API失敗\n",
      "警告呼叫API失敗\n",
      "警告呼叫API失敗\n",
      "警告呼叫API失敗\n",
      "警告呼叫API失敗\n",
      "警告呼叫API失敗\n",
      "警告呼叫API失敗\n",
      "警告呼叫API失敗\n",
      "警告呼叫API失敗\n",
      "警告呼叫API失敗\n",
      "警告呼叫API失敗\n",
      "警告呼叫API失敗\n",
      "警告呼叫API失敗\n",
      "警告呼叫API失敗\n",
      "警告呼叫API失敗\n",
      "警告呼叫API失敗\n",
      "警告呼叫API失敗\n",
      "警告呼叫API失敗\n",
      "警告呼叫API失敗\n",
      "警告呼叫API失敗\n",
      "警告呼叫API失敗\n",
      "警告呼叫API失敗\n",
      "警告呼叫API失敗\n",
      "警告呼叫API失敗\n",
      "警告呼叫API失敗\n",
      "警告呼叫API失敗\n",
      "警告呼叫API失敗\n",
      "警告呼叫API失敗\n",
      "警告呼叫API失敗\n",
      "警告呼叫API失敗\n",
      "警告呼叫API失敗\n",
      "警告呼叫API失敗\n",
      "警告呼叫API失敗\n",
      "警告呼叫API失敗\n",
      "警告呼叫API失敗\n",
      "警告呼叫API失敗\n",
      "警告呼叫API失敗\n",
      "警告呼叫API失敗\n",
      "警告呼叫API失敗\n",
      "警告呼叫API失敗\n",
      "警告呼叫API失敗\n",
      "警告呼叫API失敗\n",
      "警告呼叫API失敗\n",
      "警告呼叫API失敗\n",
      "警告呼叫API失敗\n",
      "警告呼叫API失敗\n",
      "警告呼叫API失敗\n",
      "警告呼叫API失敗\n",
      "警告呼叫API失敗\n",
      "警告呼叫API失敗\n",
      "警告呼叫API失敗\n",
      "警告呼叫API失敗\n",
      "警告呼叫API失敗\n",
      "警告呼叫API失敗\n",
      "警告呼叫API失敗\n",
      "程式運行時間：22.73秒\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from pypdf import PdfReader\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "import openai\n",
    "import pandas as pd\n",
    "start_time = time.time()\n",
    "openai.api_key = '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'\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "pdf_name = \"Unsupervised_Maritime_Vessel_Re-Identification_With_Multi-Level_Contrastive_Learning.pdf\"\n",
    "reader = PdfReader(pdf_name)\n",
    "number_of_pages = len(reader.pages)\n",
    "\n",
    "chunks = []\n",
    "\n",
    "\n",
    "translation = pd.DataFrame(columns=['原文', '翻譯結果'])\n",
    "\n",
    "for i in range(number_of_pages):\n",
    "    page = reader.pages[i]\n",
    "    text = page.extract_text()\n",
    "    sentences = sent_tokenize(text)\n",
    "    input_sentences = ''\n",
    "  \n",
    "    for sentence in sentences:\n",
    "        input_sentences += sentence\n",
    "        if len(input_sentences) > 1000:\n",
    "            chunks.append(input_sentences)\n",
    "            input_sentences = ''\n",
    "    chunks.append(input_sentences)\n",
    "    \n",
    "\n",
    "        \n",
    "for i in range(len(chunks)):\n",
    "    try:\n",
    "        completion = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"請你成為文章翻譯的小幫手，請協助翻譯以下技術文件，以繁體中文輸出\"},\n",
    "                    {\"role\": \"user\", \"content\": chunks[i]},\n",
    "                  ]\n",
    "                                                  )\n",
    "        print('='*20) \n",
    "        print('原文:\\n', chunks[i])\n",
    "        print('='*20)  \n",
    "        print('翻譯結果:\\n', completion.choices[0].message.content)\n",
    "        print('='*20)\n",
    "        translation.loc[i] = [chunks[i], completion.choices[0].message.content]\n",
    "    except:\n",
    "        print(\"警告呼叫API失敗\")\n",
    "\n",
    "translation.to_csv(pdf_name+\"translation\"+\".csv\", encoding='utf-8-sig')\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "# 計算程式運行時間\n",
    "duration = end_time - start_time\n",
    "print(f\"程式運行時間：{duration:.2f}秒\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a1856b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c571cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "translation.to_csv(pdf_name+\"translation_utf-8-sig\"+\".csv\", encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53ad21b6",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeEncodeError",
     "evalue": "'big5' codec can't encode character '\\xa9' in position 894: illegal multibyte sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtranslation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdf_name\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtranslation_big5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbig5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Read_Paper\\lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Read_Paper\\lib\\site-packages\\pandas\\core\\generic.py:3720\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3709\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_frame()\n\u001b[0;32m   3711\u001b[0m formatter \u001b[38;5;241m=\u001b[39m DataFrameFormatter(\n\u001b[0;32m   3712\u001b[0m     frame\u001b[38;5;241m=\u001b[39mdf,\n\u001b[0;32m   3713\u001b[0m     header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3717\u001b[0m     decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[0;32m   3718\u001b[0m )\n\u001b[1;32m-> 3720\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameRenderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3722\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlineterminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3723\u001b[0m \u001b[43m    \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3724\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3725\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3726\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3727\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquoting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquoting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3728\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3729\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3730\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3731\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3732\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquotechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquotechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3733\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3734\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoublequote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3735\u001b[0m \u001b[43m    \u001b[49m\u001b[43mescapechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mescapechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3737\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Read_Paper\\lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Read_Paper\\lib\\site-packages\\pandas\\io\\formats\\format.py:1189\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m   1168\u001b[0m     created_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1170\u001b[0m csv_formatter \u001b[38;5;241m=\u001b[39m CSVFormatter(\n\u001b[0;32m   1171\u001b[0m     path_or_buf\u001b[38;5;241m=\u001b[39mpath_or_buf,\n\u001b[0;32m   1172\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1187\u001b[0m     formatter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt,\n\u001b[0;32m   1188\u001b[0m )\n\u001b[1;32m-> 1189\u001b[0m \u001b[43mcsv_formatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[0;32m   1192\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Read_Paper\\lib\\site-packages\\pandas\\io\\formats\\csvs.py:261\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    241\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_handle(\n\u001b[0;32m    242\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilepath_or_buffer,\n\u001b[0;32m    243\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    249\u001b[0m \n\u001b[0;32m    250\u001b[0m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m csvlib\u001b[38;5;241m.\u001b[39mwriter(\n\u001b[0;32m    252\u001b[0m         handles\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[0;32m    253\u001b[0m         lineterminator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    258\u001b[0m         quotechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquotechar,\n\u001b[0;32m    259\u001b[0m     )\n\u001b[1;32m--> 261\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Read_Paper\\lib\\site-packages\\pandas\\io\\formats\\csvs.py:266\u001b[0m, in \u001b[0;36mCSVFormatter._save\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_need_to_save_header:\n\u001b[0;32m    265\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_header()\n\u001b[1;32m--> 266\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Read_Paper\\lib\\site-packages\\pandas\\io\\formats\\csvs.py:304\u001b[0m, in \u001b[0;36mCSVFormatter._save_body\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m start_i \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m end_i:\n\u001b[0;32m    303\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m--> 304\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_i\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_i\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Read_Paper\\lib\\site-packages\\pandas\\io\\formats\\csvs.py:315\u001b[0m, in \u001b[0;36mCSVFormatter._save_chunk\u001b[1;34m(self, start_i, end_i)\u001b[0m\n\u001b[0;32m    312\u001b[0m data \u001b[38;5;241m=\u001b[39m [res\u001b[38;5;241m.\u001b[39miget_values(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(res\u001b[38;5;241m.\u001b[39mitems))]\n\u001b[0;32m    314\u001b[0m ix \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_index[slicer]\u001b[38;5;241m.\u001b[39m_format_native_types(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_number_format)\n\u001b[1;32m--> 315\u001b[0m \u001b[43mlibwriters\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_csv_rows\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m    \u001b[49m\u001b[43mix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnlevels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Read_Paper\\lib\\site-packages\\pandas\\_libs\\writers.pyx:75\u001b[0m, in \u001b[0;36mpandas._libs.writers.write_csv_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m: 'big5' codec can't encode character '\\xa9' in position 894: illegal multibyte sequence"
     ]
    }
   ],
   "source": [
    "translation.to_csv(pdf_name+\"translation_big5\"+\".csv\", encoding='big5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdebcf5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537c8dff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d2c8320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fitz in c:\\users\\william\\anaconda3\\envs\\read\\lib\\site-packages (0.0.1.dev2)\n",
      "Requirement already satisfied: configobj in c:\\users\\william\\anaconda3\\envs\\read\\lib\\site-packages (from fitz) (5.0.8)\n",
      "Requirement already satisfied: pyxnat in c:\\users\\william\\anaconda3\\envs\\read\\lib\\site-packages (from fitz) (1.5)\n",
      "Requirement already satisfied: httplib2 in c:\\users\\william\\anaconda3\\envs\\read\\lib\\site-packages (from fitz) (0.22.0)\n",
      "Requirement already satisfied: configparser in c:\\users\\william\\anaconda3\\envs\\read\\lib\\site-packages (from fitz) (5.3.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\william\\anaconda3\\envs\\read\\lib\\site-packages (from fitz) (2.0.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\william\\anaconda3\\envs\\read\\lib\\site-packages (from fitz) (1.10.1)\n",
      "Requirement already satisfied: nibabel in c:\\users\\william\\anaconda3\\envs\\read\\lib\\site-packages (from fitz) (5.1.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\william\\anaconda3\\envs\\read\\lib\\site-packages (from fitz) (1.24.2)\n",
      "Requirement already satisfied: nipype in c:\\users\\william\\anaconda3\\envs\\read\\lib\\site-packages (from fitz) (1.8.6)\n",
      "Requirement already satisfied: six in c:\\users\\william\\anaconda3\\envs\\read\\lib\\site-packages (from configobj->fitz) (1.16.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in c:\\users\\william\\anaconda3\\envs\\read\\lib\\site-packages (from httplib2->fitz) (3.0.9)\n",
      "Requirement already satisfied: packaging>=17 in c:\\users\\william\\anaconda3\\envs\\read\\lib\\site-packages (from nibabel->fitz) (23.0)\n",
      "Requirement already satisfied: pydot>=1.2.3 in c:\\users\\william\\anaconda3\\envs\\read\\lib\\site-packages (from nipype->fitz) (1.4.2)\n",
      "Requirement already satisfied: click>=6.6.0 in c:\\users\\william\\anaconda3\\envs\\read\\lib\\site-packages (from nipype->fitz) (8.1.3)\n",
      "Requirement already satisfied: networkx>=2.0 in c:\\users\\william\\anaconda3\\envs\\read\\lib\\site-packages (from nipype->fitz) (3.1)\n",
      "Requirement already satisfied: etelemetry>=0.2.0 in c:\\users\\william\\anaconda3\\envs\\read\\lib\\site-packages (from nipype->fitz) (0.3.0)\n",
      "Requirement already satisfied: traits!=5.0,<6.4,>=4.6 in c:\\users\\william\\anaconda3\\envs\\read\\lib\\site-packages (from nipype->fitz) (6.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.2 in c:\\users\\william\\anaconda3\\envs\\read\\lib\\site-packages (from nipype->fitz) (2.8.2)\n",
      "Requirement already satisfied: rdflib>=5.0.0 in c:\\users\\william\\anaconda3\\envs\\read\\lib\\site-packages (from nipype->fitz) (6.3.2)\n",
      "Requirement already satisfied: simplejson>=3.8.0 in c:\\users\\william\\anaconda3\\envs\\read\\lib\\site-packages (from nipype->fitz) (3.19.1)\n",
      "Requirement already satisfied: filelock>=3.0.0 in c:\\users\\william\\anaconda3\\envs\\read\\lib\\site-packages (from nipype->fitz) (3.11.0)\n",
      "Requirement already satisfied: looseversion in c:\\users\\william\\anaconda3\\envs\\read\\lib\\site-packages (from nipype->fitz) (1.1.2)\n",
      "Requirement already satisfied: prov>=1.5.2 in c:\\users\\william\\anaconda3\\envs\\read\\lib\\site-packages (from nipype->fitz) (2.0.0)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\william\\anaconda3\\envs\\read\\lib\\site-packages (from pandas->fitz) (2023.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\william\\anaconda3\\envs\\read\\lib\\site-packages (from pandas->fitz) (2023.3)\n",
      "Requirement already satisfied: requests>=2.20 in c:\\users\\william\\anaconda3\\envs\\read\\lib\\site-packages (from pyxnat->fitz) (2.28.2)\n",
      "Requirement already satisfied: lxml>=4.3 in c:\\users\\william\\anaconda3\\envs\\read\\lib\\site-packages (from pyxnat->fitz) (4.9.2)\n",
      "Requirement already satisfied: future>=0.16 in c:\\users\\william\\anaconda3\\envs\\read\\lib\\site-packages (from pyxnat->fitz) (0.18.3)\n",
      "Requirement already satisfied: pathlib>=1.0 in c:\\users\\william\\anaconda3\\envs\\read\\lib\\site-packages (from pyxnat->fitz) (1.0.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\william\\anaconda3\\envs\\read\\lib\\site-packages (from click>=6.6.0->nipype->fitz) (0.4.6)\n",
      "Requirement already satisfied: ci-info>=0.2 in c:\\users\\william\\anaconda3\\envs\\read\\lib\\site-packages (from etelemetry>=0.2.0->nipype->fitz) (0.3.0)\n",
      "Requirement already satisfied: isodate<0.7.0,>=0.6.0 in c:\\users\\william\\anaconda3\\envs\\read\\lib\\site-packages (from rdflib>=5.0.0->nipype->fitz) (0.6.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\william\\anaconda3\\envs\\read\\lib\\site-packages (from requests>=2.20->pyxnat->fitz) (1.26.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\william\\anaconda3\\envs\\read\\lib\\site-packages (from requests>=2.20->pyxnat->fitz) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\william\\anaconda3\\envs\\read\\lib\\site-packages (from requests>=2.20->pyxnat->fitz) (3.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\william\\anaconda3\\envs\\read\\lib\\site-packages (from requests>=2.20->pyxnat->fitz) (2022.12.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install fitz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2890181",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fitz'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17364\\2093292272.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mfitz\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'punkt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mpdf_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'Unsupervised_Maritime_Vessel_Re-Identification_With_Multi-Level_Contrastive_Learning.pdf'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'fitz'"
     ]
    }
   ],
   "source": [
    "import fitz\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "pdf_name = 'Unsupervised_Maritime_Vessel_Re-Identification_With_Multi-Level_Contrastive_Learning.pdf' \n",
    "doc = fitz.open(pdf_name)\n",
    "paragraphs = []\n",
    "\n",
    "for page in doc:\n",
    "    text = page.get_text()\n",
    "    # 將每頁文字分割成段落或句子，以便後續翻譯\n",
    "    # 這裡使用 NLTK 的 sent_tokenize 將文字分割成句子\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    paragraphs.extend(sentences)\n",
    "\n",
    "    \n",
    "    \n",
    "import openai\n",
    "import time\n",
    "\n",
    "openai.api_key = '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'\n",
    "\n",
    "translation = []\n",
    "\n",
    "for p in paragraphs:\n",
    "    # 請稍等一下，避免 API 請求過於頻繁\n",
    "    time.sleep(1)\n",
    "    result = openai.Completion.create(\n",
    "        engine='text-davinci-002',\n",
    "        prompt=f'Translate the following sentence from English to Traditional Chinese: \\n\"{p}\"\\nTranslation:',\n",
    "        max_tokens=100,\n",
    "        n=1,\n",
    "        stop=None,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    translation.append(result.choices[0].text.strip())\n",
    "\n",
    "    \n",
    "import csv\n",
    "\n",
    "with open('translation.csv', 'w', newline='', encoding='utf-8-sig') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['Original', 'Translation'])\n",
    "    for i in range(len(paragraphs)):\n",
    "        writer.writerow([paragraphs[i], translation[i]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fea4567",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
